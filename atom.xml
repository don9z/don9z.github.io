<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Extra Cookies</title>
  
  <subtitle>By Dong ZHENG</subtitle>
  <link href="http://blog.zhengdong.me/atom.xml" rel="self"/>
  
  <link href="http://blog.zhengdong.me/"/>
  <updated>2021-04-17T16:23:35.593Z</updated>
  <id>http://blog.zhengdong.me/</id>
  
  <author>
    <name>Dong Zheng</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>读情境领导者和钝感力</title>
    <link href="http://blog.zhengdong.me/2021/04/11/situational-leader-and-insensitivity-power/"/>
    <id>http://blog.zhengdong.me/2021/04/11/situational-leader-and-insensitivity-power/</id>
    <published>2021-04-11T05:31:20.000Z</published>
    <updated>2021-04-17T16:23:35.593Z</updated>
    
    <content type="html"><![CDATA[<p>最近看书，有两本印象不错。</p><p>*** ***</p><p>第一本是「<a href="https://book.douban.com/subject/1102987/">情境领导者</a>」，比较老的书，是一套帮助领导者提高领导行为有效性的模式：有效的领导风格需要和被领导者的准备度水平相匹配。</p><p>这方面知识很多管理类书都会提及，这本书给我带来的最大收获在于做了系统化拆解，可以说以前是基于内心认知来实践，书中的观点更具象，更可被衡量。</p><p>首先是领导者风格的定义：1）领导者采用的<strong>工作行为</strong>的数量；2）领导者采用的<strong>关系行为</strong>的数量。</p><ul><li><strong>工作行为</strong>：是指领导者清楚的说明个人或组织的责任的程度。这种行为包括告诉人们做什么，如何做，什么时间做，在哪里做，以及由谁来做。</li><li><strong>关系行为</strong>：是指当管理对象超过一个人的时候，领导者进行双向或者多向沟通的程度。这种行为包括倾听、鼓励、协助、提供工作说明以及给予社交方面的支持等。</li></ul><p>基于这两类行为的定义，可以将领导风格划分四类：</p><ul><li>S1: 高工作，低关系：命令型 (<strong>telling</strong>)，提供明确的说明和密切的监管。</li><li>S2: 高工作，高关系：指导型 (<strong>selling</strong>)，解释你的决策并给予对方要求陈述的机会。</li><li>S3: 低工作，高关系：鼓励型 (<strong>participating</strong>)，交换意见并辅助被领导者制定决策。</li><li>S4: 低工作，低关系：授权型 (<strong>delegating</strong>)，将决策和执行的职责交给员工。</li></ul><p>接下来是对被领导者的准备度水平的定义：指被领导者完成某项特定的工作所表现出来的<strong>能力</strong>和<strong>意愿</strong>水平。</p><ul><li><strong>能力</strong>是指个人或组织在某一项特定的工作或活动中所表现出的知识、经验与技能。</li><li><strong>意愿</strong>是指个人或组织完成某一项特定的工作或活动而表现出的信心、承诺和动机。</li></ul><p>同样基于这两项，将被领导者准备度水平划分四类：</p><ul><li>R1: 没能力 &amp; 没意愿或不安</li><li>R2: 没能力 &amp; 有意愿或自信</li><li>R3: 有能力 &amp; 没意愿或不安</li><li>R4: 有能力 &amp; 有意愿并自信</li></ul><p>最后是领导风格和被领导者准备度水平的匹配关系，S和R一一对应（R1、R2 领导者主导，R3、R4 被领导者主导），见下图。</p><p><a><img src="/images/2021/the-situational-leader.png" width="560"></a></p><p>*** ***</p><p>第二本书是渡边淳一 的「<a href="https://book.douban.com/subject/2119843/">钝感力</a>」，是本励志畅销书。整本书从人生/职场发展，人际关系，身体健康，男女关系等各方面，举各种例子来讲述一个道理：虽然钝感有时给人以迟钝、木讷的负面印象，钝感力却是我们赢得美好生活的手段和智慧。</p><p>整本读起来很容易，读的过程也再三告诉自己，不要较真，这不是本理论或者教科书，而是作者讲述自己人生哲学的小册子。</p><p>我的收获：感官上的钝感和敏感，和人格特质关系很大，很多时候不好改变，所以“钝感力”更是在对失败、挑战、焦虑、诱惑等的应对上要钝感，这是可以被训练的。钝感力的最终表现是对不确定性的接纳和消化能力，也就是”<a href="https://book.douban.com/subject/25782902/">反脆弱</a>”的能力。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近看书，有两本印象不错。&lt;/p&gt;
&lt;p&gt;*** ***&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>对 DataOps 的一些理解</title>
    <link href="http://blog.zhengdong.me/2021/03/10/about-dataops/"/>
    <id>http://blog.zhengdong.me/2021/03/10/about-dataops/</id>
    <published>2021-03-10T03:50:11.000Z</published>
    <updated>2021-03-13T07:50:10.038Z</updated>
    
    <content type="html"><![CDATA[<p>最近看了一些关于 DataOps 的书和文章，对自己的理解做个总结。</p><h2 id="dataops-要解决什么问题">DataOps 要解决什么问题？</h2><p>两个价值点，第一是<strong>提高数据生产到价值产出全链路的效率</strong>，第二是<strong>用数据来持续优化整个数据链路的可用性和价值</strong>。</p><h2 id="dataops-是什么">DataOps 是什么？</h2><p>下面是 Gartner 对 DataOps 的<a href="https://www.gartner.com/en/information-technology/glossary/dataops">定义</a>。</p><blockquote><p>DataOps is a collaborative data management practice focused on improving the communication, integration and automation of data flows between data managers and data consumers across an organization.</p></blockquote><p>这里包含三份信息：</p><ol type="1"><li>DataOps 是包括人，流程和技术的一组体系和实践，用来管理代码，工具，基础架构和数据本身，</li><li>它针对的是数据生产者和数据消费者间的整个数据流（价值链路），</li><li>它持续优化链路上参与方的协作，集成和自动化能力。</li></ol><p>接下来的描述，指出了 DataOps 理念的的三个支柱：</p><blockquote><p>The goal of DataOps is to deliver value faster by creating predictable delivery and change management of data, data models and related artifacts.</p></blockquote><p>敏捷，</p><blockquote><p>DataOps uses technology to automate the design, deployment and management of data delivery with appropriate levels of governance,</p></blockquote><p>自动化，</p><blockquote><p>and it uses metadata to improve the usability and value of data in a dynamic environment.</p></blockquote><p>和持续优化。</p><p>基于这些支柱，DataOps 构建和持续优化两个工作流。一个是 “<strong>The Value Pipeline</strong>”，覆盖数据接入，ETL、建模、可视化/报表整个流程，我认为可以抽象成“<strong>建好数据</strong>”。第二是 “<strong>The Innovation Pipeline</strong>”，流程包括从想法、到研发、再到生产验证，我认为可以抽象为“<strong>用好数据</strong>”。</p><p>DataOps 对“建好数据”流程进行全链路提效，以及通过数据反向优化进一步提高效率和质量。为“用好数据”流程提供高效的开发和试验工具，以及多环境支持（Sandbox或预发、测试环境），高效试验不影响线上。</p><h2 id="如何来落地-dataops">如何来落地 DataOps？</h2><p>从人、流程和技术的三方面来看。</p><p><strong>DataOps 的客户是谁？</strong></p><p>是整个数据团队，包括了产品经理，数据研发，工程开发，测试，运维，数据科学，数据分析师等人员。</p><p><strong>数据生产者到数据消费者的流程包括哪些环节？</strong></p><p>1）开发阶段，目标是持续集成；2）部署阶段，目标是持续部署；3）编排阶段，是解决工作流自动化和调度问题；4）测试阶段，建立监控和告警机制，目标是提升数据的质量。</p><p><strong>涉及到的数据技术有哪些？</strong></p><p>1）数据采集：采集和整合异构数据；2）数据存储：基于计算基础设施之上的数仓或数据湖；3）数据生产：数据开发、优化、测试等技术；4）数据管理：元数据管理和数据治理技术；5）数据分析：数据报表，可视化，数据科学工具等技术。</p><p>基于以上技术，在敏捷、自动化和持续优化的加持下，要建立四个能力：数据工程（处理和加工数据的能力），数据集成（多样、异构数据的整合能力），数据安全（端到端的数据安全和隐私管控能力）和数据质量（数据的一致性、准确性、及时性等数据质量保障能力）。</p><p>摘抄 <a href="https://datakitchen.io/the-dataops-cookbook/">The DataOps Cookbook</a> 书中的一些内容：</p><blockquote><p>The Seven Steps to Implement DataOps</p><ol type="1"><li>Add data and logical tests</li><li>Use a version control system</li><li>Branch and merge</li><li>Use multiple environments</li><li>Reuse and containerize</li><li>Parameter your processing</li><li>Working without fear or heroism</li></ol></blockquote><p>总结起来，就是将数据纳入测试，提供分支和多环境能力，自动化执行，且便于复用。</p><blockquote><p><a href="https://www.dataopsmanifesto.org">The DataOps Manifesto</a></p><ol type="1"><li>Continuously satisfy your customer</li><li>Value working analytics</li><li>Embrace change</li><li>It’s a team sport</li><li>Daily interactions</li><li>Self-organize</li><li>Reduce heroism</li><li>Reflect</li><li>Analytics is code</li><li>Orchestrate</li><li>Make it reproducible</li><li>Disposable environments</li><li>Simplicity</li><li>Analytics is manufacturing</li><li>Quality is paramount</li><li>Monitor quality and performance</li><li>Reuse</li><li>Improve cycle times</li></ol></blockquote><p><strong>书中关于作为CDO如何让数据的价值被CEO认可和信任，这其实就是数据人的目标：</strong></p><blockquote><p>CAOs and CDOs: Earn the Trust of your CEO</p><ol type="1"><li>Earn trust by delivering a journey of value</li><li>Earn trust by delivering quickly</li><li>Earn trust by delivering accurately</li></ol></blockquote><p><strong>获得信任，首先要做到数据是有价值的，并且做到快速交付，高质量交付。</strong></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近看了一些关于 DataOps 的书和文章，对自己的理解做个总结。&lt;/p&gt;
&lt;h2 id=&quot;dataops-要解决什么问题&quot;&gt;DataOps 要解决什么问题？&lt;/h2&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>如何做产品定位 - 「Obvious Awesome - How to nail product pos」</title>
    <link href="http://blog.zhengdong.me/2021/02/16/how-to-nail-product-pos/"/>
    <id>http://blog.zhengdong.me/2021/02/16/how-to-nail-product-pos/</id>
    <published>2021-02-16T07:51:23.000Z</published>
    <updated>2021-02-16T08:02:43.581Z</updated>
    
    <content type="html"><![CDATA[<p>书中关于定位的表述，相比著名的 <a href="https://www.amazon.com/Positioning-Battle-Your-Al-Ries/dp/0071373586/ref=sr_1_1?dchild=1&amp;keywords=positioning&amp;qid=1613460532&amp;sr=8-1">Positioning: The Battle for Your Mind</a>，更产品视角，不仅仅是讲述如何在营销战中抓住用户获取胜利，而是如何基于定位去打造产品、迭代产品。</p><p>看完 <a href="https://www.amazon.com/Obviously-Awesome-Product-Positioning-Customers/dp/1999023005/ref=sr_1_1?dchild=1&amp;keywords=obviously+awesome&amp;qid=1613461641&amp;sr=8-1">Obvious Awesome</a>，我理解的重点有三部分，一是究竟什么是定位（Positioning），二是有效定位有哪些组成部分，三是如何通过10个步骤来确定定位。</p><h2 id="什么是定位">什么是定位</h2><p>引言中作者对定位是这么描述的：</p><blockquote><p>Positioning is the act of deliberately defining how you are the best at something that a defined market cares a lot about.</p></blockquote><p>之后用上下文（Context）来做类比：</p><blockquote><p>Context enables people to figure out what’s important. Positioning products is a lot like context setting in the opening of a movie.</p></blockquote><p>作者用电影来举例，比如开场几分钟会让观众大体获知影片的一些关键信息，比如是悲喜剧还是惊悚片。我的理解，产品的定位，就是提供一系列上下文，让客户在第一次看到产品的时候，能理解这个产品是什么，能做什么，他应该关心什么。</p><h2 id="有效产品定位的5个组成部分">有效产品定位的5个组成部分</h2><p><strong>1）Competitive alternatives</strong></p><p>如果我们的解决方案不存在，客户会用什么方式来替代？</p><p><strong>2）Unique attributes</strong></p><p>我们有，但替代方式（竞品）缺少的特性和能力是什么？</p><p><strong>3）Value (and proof)</strong></p><p>这些独有特性能为客户带来哪些价值？</p><p><strong>4）Target market characteristics</strong></p><p>真正关心产品价值的典型客户群特征是什么？</p><p><strong>5）Market category</strong></p><p>为了让客户理解我们提供的价值，我们将产品列入的市场类别是什么？</p><p><strong><em>6）(Bonus) Relevant trends</em></strong></p><p>有哪些行业/市场趋势，能帮助我们让目标客户更好建立对我们产品的兴趣？</p><h2 id="确定定位的10个步骤">确定定位的10个步骤</h2><p><strong>1）理解热爱我们产品的客户</strong></p><p>有效定位的5个组成部分是相互依赖的，环环相扣，该如何开始？</p><p>首先是列出产品最热爱客户的短名单。和这些客户交流，能帮助我们聚焦，找到客户为选择我们是否存在典型模式。如果当前还找不到这些客户，那暂将产品的定位约束放松，先通过产品的迭代和目标客户定向找到这些客户。</p><p><strong>2）组建定位团队</strong></p><p>需要哪些岗位参加进来？带来哪些输出？</p><ul><li><strong>公司负责人（或大公司事业部负责人）</strong> -&gt; 整体的业务策略</li><li>市场 -&gt; 营销信息，受众定位，营销活动</li><li>销售和BD -&gt; 目标客户细分，销售侧客户策略</li><li>客户成功 -&gt; 交付和客户拓展策略</li><li>产品和研发 -&gt; 里程碑和优先级</li></ul><p>组织形式：</p><ul><li>人不宜过多，但每组都需要有人在，才能达成真正共识</li><li>公司外人员来 facilitate 讨论，可以让讨论更高效和平衡</li></ul><p><strong>3）统一定位词汇，去除定位包袱</strong></p><p>统一概念和定义：何为定位、为何重要，有哪些组成部分、如何定义，市场成熟度和竞争环境如何影响产品定位方式？</p><p>为了找出对产品可能新的理解方式，我们需要刻意<strong>摆脱固有思考方式及固有观点</strong>。</p><p><strong>4）列出真正的竞品</strong></p><p>客户和我们看待竞品的方式经常不一样，而他们的观点才是我们定位唯一所关注的。</p><p>找寻竞品最好的办法是回答这个问题：<strong>如何我们的产品不存在，最热爱我们产品的客户会如何做？</strong></p><p><strong>5）分离出我们独有的特性</strong></p><p>有了竞品后，下一步就是分析，相比竞品，是什么让我们不同和更好的？</p><p>需要注意的几个点：</p><ol type="1"><li>列出所有我们有但竞品没有的能力；</li><li>避免使用观点性描述除非有明确事实证明，如“我们提供优秀的客户服务”，“我们有最好的易用性”等；</li><li>聚焦在获客相关特性上，留存相关特性并非不重要，而是只有当客户真正了解我们后，才会发挥作用。</li></ol><p><strong>6）将特性映射到价值场景</strong></p><p>独有特性是起点，但客户更关心的是这些特性对他们有什么价值？</p><p>书中特性到价值映射的举例，从特性（features）到用途（benefits）到客户价值（value）：</p><ul><li>特性 -&gt; your product does or has: One-clink reports</li><li>用途 -&gt; features enable for customers: fast, easy report generation</li><li>客户价值 -&gt; customer is trying to achieve: every part of the organization can make better decisions based on accurate, up-to-date metrics.</li></ul><p><strong>7）确定哪些特性/价值是我们最关心的</strong></p><p>理解产品相比竞品的独有价值后，我们要去找寻客户真正关心哪些价值。</p><p>目标客户群要尽可能聚焦，之后可以扩展更多客户群。两个前提</p><ul><li>该客户群规模足够满足我们的短期商业目标</li><li>该客户群有一致的未解决痛点</li></ul><p><strong>8）选择能让我们胜出的市场类别</strong></p><p>三种选择：</p><ol type="1"><li>Head to Head：定位在既有市场类别，目标成为Top玩家</li><li>Big Fish, Small Pond：定位在既有市场类别，目标是在细分类目胜出</li><li>Create a New Game：建立新市场列别，目标成为Top玩家</li></ol><p><strong>9）借助趋势</strong></p><p>如果有趋势，有风口，我们应该借力，但如果没有，是不影响产品的成功的。</p><p><strong>10）记录定位，用于分享</strong></p><p>定位确认后，需要通过在组织内部的分享，<strong>得到大家的 buy-in，才能被用在品牌传播，营销活动，销售策略，产品决策，客户成功策略中</strong>，才算落地。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;书中关于定位的表述，相比著名的 &lt;a href=&quot;https://www.amazon.com/Positioning-Battle-Your-Al-Ries/dp/0071373586/ref=sr_1_1?dchild=1&amp;amp;keywords=positioning&amp;amp;qid=1613460532&amp;amp;sr=8-1&quot;&gt;Positioning: The Battle for Your Mind&lt;/a&gt;，更产品视角，不仅仅是讲述如何在营销战中抓住用户获取胜利，而是如何基于定位去打造产品、迭代产品。&lt;/p&gt;
&lt;p&gt;看完 &lt;a href=&quot;https://www.amazon.com/Obviously-Awesome-Product-Positioning-Customers/dp/1999023005/ref=sr_1_1?dchild=1&amp;amp;keywords=obviously+awesome&amp;amp;qid=1613461641&amp;amp;sr=8-1&quot;&gt;Obvious Awesome&lt;/a&gt;，我理解的重点有三部分，一是究竟什么是定位（Positioning），二是有效定位有哪些组成部分，三是如何通过10个步骤来确定定位。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>数据产品增长问题的一个分析框架</title>
    <link href="http://blog.zhengdong.me/2021/01/15/data-product-growth-analysis-framework/"/>
    <id>http://blog.zhengdong.me/2021/01/15/data-product-growth-analysis-framework/</id>
    <published>2021-01-15T14:48:24.000Z</published>
    <updated>2021-01-15T15:02:20.975Z</updated>
    
    <content type="html"><![CDATA[<p><strong>一、从客户的角度来看，我们的客户都是谁？他们的核心痛点是什么？</strong></p><p>我们的客户可能是分析师，业务人员，企业决策层，企业IT团队，甚至企业服务的客户，不同客户的痛点是不同的，进而对产品的期望也不同。这里会面临两个决策点：</p><ol type="1"><li>确定我们真正要服务的客户是哪些？让所有人满意是非常难的。</li><li>对这些客户按角色、数据能力等进行分层，针对定义服务的方式。</li></ol><p>举例：基于客户分层的产品服务方式</p><ul><li>让他们可以快速获知自己关注指标的状态，帮助他们迅速获知面临哪些问题。</li><li>让他们能够在特定业务场景的分析框架下，灵活定义自己的看数视角，获得洞察。</li><li>让他们能够定义自己的分析框架，自助取数和分析。</li></ul><p><strong>二、从交易的角度来看，我们为客户提供了哪些让他们持续使用的价值？我们能否持续高效的创造这些价值，且成本足够低？</strong></p><p>产品提供的功能很多，也许都有用，但不同功能对客户的吸引力是不同的。这里要弄清楚两个问题：</p><ol type="1"><li>找到产品吸引用户使用的价值点究竟有哪些？分析各个价值点对用户的粘性。</li><li>从生产成本的角度来评判这些价值，由我们提供是否合适？即回答”为什么是我们“这个问题。（产品3问：1. 我们的客户是谁？2. 我们能为我们的客户做什么？3. 为什么是我们？）</li></ol><p>举例：基于价值分层的产品需求分类</p><ul><li>底线需求：一定要做。</li><li>够用就好：适可而止，控制成本。</li><li>越多越好：持续投入，但生产端要提效降本。</li><li>惊喜：阶段性适量提供。</li></ul><p><strong>三、从平台的角度来看，我们的运营和产品分发体系是否足够精准让对的人看到对的内容？我们是否有一套内容价值评估和汰换机制？</strong></p><p>平台提供的服务趋向多样化是平台内生的动力，供和需的多方参与，会带来复杂性和问题。有两件事是要去做：</p><ol type="1"><li>供需匹配，建立内容的精准分发能力，基于推荐或定点运营，持续做平台提效。</li><li>反馈系统，建立一套内容价值评估和反馈收集能力，基于此做汰换，持续做平台瘦身。</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;一、从客户的角度来看，我们的客户都是谁？他们的核心痛点是什么？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;我们的客户可能是分析师，业务人员，企业决策层，企业IT团队，甚至企业服务的客户，不同客户的痛点是不同的，进而对产品的期望也不同。这里会面临两个决策点：&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>关于未来内容型数据产品方向的几个观点</title>
    <link href="http://blog.zhengdong.me/2020/12/06/view-of-the-future-data-product-trends/"/>
    <id>http://blog.zhengdong.me/2020/12/06/view-of-the-future-data-product-trends/</id>
    <published>2020-12-06T15:54:42.000Z</published>
    <updated>2020-12-08T16:27:13.662Z</updated>
    
    <content type="html"><![CDATA[<p>这里的内容型数据产品指的是以提供内容为主要目标的数据产品，区别于以提供技术能力为主要目标的工具型数据产品，如 Tableau 及类似的可视化 BI 产品、Zeppelin 及类似的分析、协作产品等。</p><h2 id="内容型数据产品的3个方向">内容型数据产品的3个方向</h2><p>未来内容型数据产品方向，我认为是如下三个：</p><ol type="1"><li>数据产品基于BI或低代码工具搭建而成；</li><li>数据产品和业务产品合二为一；</li><li>交互式、对话式分析成为数据产品的基本能力。<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li></ol><h3 id="一数据产品基于bi或低代码工具搭建而成">一、数据产品基于BI或低代码工具搭建而成</h3><p>我们做一个数据分析报告，或者数据探索的目的是什么？我的理解是做4件事，<strong>描述</strong>业务发生了什么，<strong>诊断</strong>为什么发生，<strong>预测</strong>将要发生什么，以及<strong>决策</strong>要做什么。</p><p>常见的内容型数据产品主要覆盖描述和诊断环节，首先让大家看清数，看懂数，才能更好的决策，更好做行动计划。看清数，要做到数据能反应业务现状，看懂数，要做到数据包含体系化分析思路。看起来不难，可现状是什么？</p><p>业务的打法和重点策略在竞争中会经常调整，对数据产品而言，需求多变且交付时间紧是常态，如果业务的策略调整等一两个月后产品才有数看，何谈看清数。</p><p>数据产品在建设初期会有明确的产品逻辑设计，但在后期的迭代过程很多走向了功能堆砌，毕竟需求都是不断在增加的，毕竟推翻重构时间上决不允许，“又不是看不到数？”。带来的问题是用户分析一个问题，可能需要若干次产品功能跳转，很难看懂数。</p><p>现在很多 BI 工具都提供了产品搭建能力，数据产品的开发过程可以转化为：1）制作可视化报表，2）设计报表的组织形式，3）独立产品发布，整个过程都是基于可视化操作和配置而成，无需独立开发。另外，低代码平台这两年也很热，通过低代码平台可视化搭建的能力组织分析报表也是一个途径。</p><p>这样带来了两个好处，1）基于搭建能力，可以做到快速响应需求进行功能新增和调整，大大缩短业务策略调整和能看清数之间的时间消耗；2）产品的组织逻辑调整不再依赖产品经理和各端开发工程师的协同，可以做到基于不同时期的分析框架来快速重组产品，让企业构建统一分析体系并迭代成为可能。</p><p>那看起来内容型数据产品变成了一个个数据报表的载体，如果这样，BI分析师是不是比数据产品经理更适合做内容型数据产品的产品经理？</p><p>是的。但我觉得内容型数据产品不应该仅仅只是描述业务发生了什么，不仅仅只是助力用户去诊断为什么发生。</p><h3 id="二数据产品和业务产品合二为一">二、数据产品和业务产品合二为一</h3><p>内容型数据产品的基本能力是以产品化的形态，帮助用户看清业务现状，引导用户洞察出业务面临的问题，做不到、做不好，都是不应该的。</p><p>但就像“懂得很多道理，却依然过不好这一生“一样，对用户而言，当看清、看懂数后，面临的最大问题往往是谁能告诉我该做什么？可能会发生什么？能怎么去做？</p><p>在一个企业内部，除了数据产品外，还有很多”后台“产品，比如进销存、供应链、广告投放、CRM等系统，权且称做业务产品，大量业务策略的落地是靠各部门的业务人员操作这些产品达成的。一个数据驱动业务的决策流程如下：通过内容型数据产品获得洞察，业务线进行理解、讨论和决策，业务线制定行动计划，业务人员操作业务产品做执行，通过内容型数据产品进行效果追踪。</p><p>整个流程涉及到多方协同，大家对数据的理解程度可能不一，涉及多个系统，数据链路也可能割裂，整体效率低，反馈链路长。存在这些问题恰恰是数据产品的机会，不应该独守一隅，要打通数据和行动，建立诊断、行动、效果回收、诊断的闭环，也许以后不存在独立的数据产品，业务产品本该就能做到用数据描述现状，基于业务诊断问题并制定计划，做行动，做效果评估和优化调整，典型的<a href="https://en.wikipedia.org/wiki/PDCA">PDCA</a>循环：Observation、Plan、Do、Check、Adjust。</p><p>举个例子，产品化支持增长框架 AARRR，除了支持新客和激活，留存，NPS，收入等分析能力外，还能做什么？本质上，AARRR 是一个拆解逻辑，每一项是可被独立分析、诊断及优化的。拿新客诊断举例，比如发现整体拉新效果不好，诊断到是因为有几个渠道转化率太低，产品能给出策略建议，比如需要调整渠道的投放分布，用户 review 后直接通过产品的行动点操作即完成整个流程。</p><p>我想以后数据驱动业务的产品体系是这样的，顶层是服务决策层的决策数据产品，对，这是内容型数据产品，下面是数据和业务融合的新型业务产品，分析和诊断形成的洞察可以直接做业务行动，业务的策略反向可以促进分析和诊断模型的优化和构建，再下面是数据资产。</p><p>BI分析师做好决策数据产品，数据开发工程师做好数据资产，数据和业务产品合二为一才是值得数据产品经理去深耕和发挥价值的领域。</p><p>数据和行动闭环的打通，也为真正做到数据智能打下了基础。</p><h3 id="三交互式对话式分析成为数据产品的基本能力">三、交互式、对话式分析成为数据产品的基本能力</h3><p>交互式和对话式分析是指数据产品不再是固化的展示数据指标，而是开放更大灵活性给用户，用户参与其中，通过交互选择或对话问问题的方式，定义自己的看数视角，获得洞察。</p><p>以用户行为转化漏斗分析举例，常规数据产品会首先根据业务诉求定义转化过程重要节点，数据开发进行需求开发，然后通过数据的可视化展现服务用户。而交互式分析模式首先是对转化分析方式进行抽象：转化漏斗分析是对漏斗窗口期内，所有满足限制条件的用户行为，按既定步骤顺序的转化计算，以漏斗图的可视化形式展现。产品模块定义如下：1）漏斗名称设置组件，2）漏斗窗口周期设置组件，3）漏斗步骤设置模块，其中每项步骤包含用户行为选择和限制条件配置，4）漏斗图展现组件。至于对哪些行为做分析，是否需要对该行为再做条件筛选，关联多长时间的数据做时间序列追踪等，交由用户选择，即席查询。对话式分析，是指这一过程用语言或语音对话的方式来进行，用户可以通过对话主动触发，平台也可以基于智能判断，推荐用户可关注的问题，比如 Google Analytics 的 Insights 功能。</p><p>这两种分析模式，对技术会带来很多挑战，如何做到千人千面的分析查询都能快速的反馈结果，如何做到很好的自然语言理解和推断能力，尤其在数据分析这个需要很多业务上下文知识，对语义歧义容忍度差的领域，等等。但技术在发展，这些问题迟早会被解决。</p><p>对产品而言的挑战的是什么？前面我们看到很多“灵活”字样，做过产品的应该知道，这暗含另一个问题，即灵活意味着需要用户 Input，意味着用户一开始可能不会用，也就是产品拥有很高的上手门槛。所以，泛化的交互式、对话式分析产品，泛化的数据智能产品，往往很难落地。做不出，做出来效果不好，做出来效果好但用户觉得不会用。那怎么办？回到做数据的一句老话，“数据的价值必须来自场景”，可以先从聚焦场景出发，比如先把用户行为分析这一个场景做透。</p><p>数据分析的大众化，数据科学的平民化，一定是趋势和未来。数据分析大众化的趋势，就是要降低数据获取和分析的难度，做到不依赖分析师，不依赖数据开发工程师，交互式分析是要解这个问题。数据科学平民化的趋势，就是要降低数据探索和洞察的难度，不需要用户既懂数据，又懂算法，对话式分析是要解这个问题。</p><h2 id="内容型数据产品经理的3个核心能力">内容型数据产品经理的3个核心能力</h2><p>前面讲了内容型数据产品的三个方向：1）数据产品基于BI或低代码工具搭建而成，2）数据产品和业务产品合二为一，3）交互式、对话式分析成为数据产品的基本能力。对于内容型数据产品经理而言，在变迁过程中如何持续保持个人竞争力，我认为最重要的是如下三项能力。</p><p><strong>1、对所服务业务的目标评估体系有好的认知</strong></p><p>先回到做产品的灵魂三问：1）我们的客户是谁？2）我们能为我们的客户做什么？3）为什么是我们？内容型数据产品的客户是谁？一开始一定是业务 Leader，得解决他的问题，他满意才有落地可能。我们能为他做什么？让他看清数，在清晰了解现状和环境的前提下做决策。为什么是我们？我们做出的产品体验优秀人人都能无脑使用？我们做出的产品拥有非常酷炫的可视化效果？都不是，是让业务 Leader 能看清数。而要让他看清数，关键依赖于你对业务目标及其评估体系的认识，哪怕分析结论是朴素的一张张表格，还是那种巨长的中国式报表，但能让现状和环境被看清，已经很优秀。接下来才是去选择合适的数据可视化形式，让用户看懂数。</p><p><strong>2、能基于业务现状和商业逻辑抽象分析框架和行动点</strong></p><p>数据和业务产品的融合能发挥效力，对产品设计者的要求我总结了9个字：知重点，有框架，能行动。知重点是指基于对业务商业逻辑和现状的认知，能定义当前重点问题；有框架是指熟知主流的分析框架，如增长框架 AARRR、用户营销模型 AIPL 等，且具备基于业务构建分析框架的能力，分析框架为的就是统一语境和思路，拆解问题，分而治之；能行动是指具备数据洞察可行动的机制建设能力，促进通过数据洞察得出的策略在业务落地，如做新客增长可能需要建立对投放系统的反馈机制，做留存复购可能需要对接权益投放系统支持权益的动态分发。</p><p><strong>3、对产品迭代的提效有精益求精的执念</strong></p><p>内容型数据产品的迭代提效，不只是开发工程师的事情，数据产品经理作为产品 Owner，要时刻想着如何更快把产品做出来，因为需求一定会变，甚至反复，时间一定会很紧，比如以小时计。在做产品设计时，对这些问题要有预判，针对性确定产品方案。</p><section class="footnotes" role="doc-endnotes"><hr /><ol><li id="fn1" role="doc-endnote"><p>本文仅代表个人观点。<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">&lt;p&gt;这里的内容型数据产品指的是以提供内容为主要目标的数据产品，区别于以提供技术能力为主要目标的工具型数据产品，如 Tableau 及类似的可视化 BI 产品、Zeppelin 及类似的分析、协作产品等。&lt;/p&gt;
&lt;h2 id=&quot;内容型数据产品的3个方向&quot;&gt;内容型数据产品的3个方向&lt;/h2&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>关于未来数据开发技术方向的几个观点</title>
    <link href="http://blog.zhengdong.me/2020/12/05/view-of-the-future-data-dev-technology/"/>
    <id>http://blog.zhengdong.me/2020/12/05/view-of-the-future-data-dev-technology/</id>
    <published>2020-12-05T12:17:48.000Z</published>
    <updated>2020-12-05T16:55:21.308Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据开发技术的3个方向">数据开发技术的3个方向</h2><p>未来数据开发技术方向，我认为有三个，首先是流批一体成为主流开发模式，其次是代码自动化技术走向成熟，第三是 OLAP Cubes 终将衰落。<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p><h3 id="一流批一体成为主流开发模式">一、流批一体成为主流开发模式</h3><p>先说说我看到的数据开发的历史。</p><ol type="1"><li>“远古”时代，通过写 SQL 脚本抽取 OLTP 数据库中数据进行分析和统计，大量查询有可能把数据库拖挂；</li><li>OLAP 分析成为数据库的一项重要能力，这个时候，可以写 SQL，也可以写 Python 代码等来进行数据分析和统计，但面对不断增长的数据量，数据库性能遇到挑战；</li><li>Hadoop 技术的引入和不断成熟，海量数据的离线存储、计算和调度问题得到解决；</li><li>Storm 让海量数据的实时计算成为可能，促进了一大批实时数据产品的出现，也促进了 Lambda 数据架构的出现和流行；</li><li>Kafka、Spark、Flink 等技术的流行，整个数据链路的全流式计算成为可能，Kappa 架构出现和流行。</li></ol><p>从单机 OLAP 到 Lambda 到 Kappa 的演进，数据链路上的问题、数据计算层面的问题得到了很好解决。那未来一切皆流式，一切皆实时是否可行？是否经济？我们的数据架构还存在什么问题？</p><p>列举几个数据领域常见的问题：</p><ul><li>数据产品实时和离线模块同一指标数值不同，因为指标计算离线、实时是单独开发，单独存储的，口径有差异；</li><li>同一口径的数据指标，需要离线和实时各开发一份代码，因为彼此的计算引擎不同，编程范式不同，即使都用 SQL 编写，也很难完全复用；</li><li>离线数仓和实时数仓彼此独立存在，带来双重的存储和维护成本，因为离线和实时任务有不同的数仓分层及存储体系。</li></ul><p>要解决这些问题，需要实时和离线计算的融合，称作流批一体架构。这里说的融合不是用实时计算替代所有离线计算，离线和实时计算有各自适用的场景，而是对于数据的下游应用来说，不用去关注数据来自实时还是离线，对于数据开发工程师来说，不用去关注开发的是实时还是离线代码，实时、离线只是调度层面的概念。融合过程根据现状的不同可以分两个阶段，第一是存储统一，第二是代码统一。</p><h4 id="实时离线的存储统一">实时、离线的存储统一</h4><p>这个阶段，实时和离线任务还是单独开发和执行的，但不再区分存储介质，比如常见的离线结果存 MySQL，实时结果存 HBase 方案，改成统一存储到海量数据高频读写皆优的存储计算引擎，如 Apache <a href="https://kudu.apache.org">Kudu</a> &amp; <a href="https://impala.apache.org">Impala</a>，<a href="https://www.alibabacloud.com/product/hologres">Alibaba Hologres</a> 等。</p><p>存储统一可以解决下游应用需要通过不同逻辑对接实时、离线数据的问题，例如统一后，同一张表取当天的数据就是截止目前为止的实时数据，取昨天的数据，就是 T-1 的离线数据。另外，也为后续第二阶段的代码统一做了铺垫，因为已经做到实时、离线统一存储，代码统一过程对下游是无感知的。</p><h4 id="实时离线的代码统一">实时、离线的代码统一</h4><p>实时和离线代码被统一为一份，通过调度设置来区分是实时还是离线批处理。这个阶段存在两个挑战。</p><p>第一个挑战是对计算引擎的，需要实时计算引擎兼具批任务执行能力，且做到流批 API 的统一，这块能力在 <a href="https://flink.apache.org/roadmap.html">Flink Roadmap</a> 里，Blink 当前已经支持的不错。</p><p>第二个挑战是对数仓架构的，实时和离线数仓需要统一，存在两个问题，1）新流批一体任务如何和所替代的老的离线任务保持统一的上游依赖？这个问题在存储、计算分离的计算平台架构下比较容易解决，打通元数据，不同计算引擎访问统一存储；2）流批一体任务依赖的上游任务可能未做流批一体，比如为了更精准的反作弊，需要独立开发离线任务通过长周期历史数据做计算，即如何解决流批一体任务依赖双重上游数据输入的问题？这个问题可以通过对该上游任务的结果表在 Schema 层面做统一来解决，如构建镜像表，根据调度模式的不同来映射依赖上游哪份数据。</p><h3 id="二代码自动化技术走向成熟">二、代码自动化技术走向成熟</h3><p>代码自动化，有两个方向，一是代码的自动生成，二是代码的自动优化。</p><h4 id="代码的自动生成">代码的自动生成</h4><p>数仓模型设计的实施工作流包含业务和需求调研，架构设计，规范定义，数据建模四个过程。</p><ul><li>架构设计指以维度建模为理论基础，基于业务和需求调研的结果，进行整体数仓设计，包含确定数据域，定义每个数据域下面的业务过程及相关维度两件事情；</li><li>规范定义指定义指标体系，包括原子指标，业务限定如修饰词、时间周期，以及派生指标，由原子指标和业务限定组合而成；</li><li>数据建模主要包括维度及属性的规范定义，维表、明细事实表和汇总事实表的模型设计。</li></ul><p>现在已经有数据研发平台可以做到可视化数据模型设计：配置化定义维度、业务过程和事实表元数据，自动生成维表和事实表；可视化关联字段的原子指标和业务限定，配置化定义派生指标口径，自动生成汇总表。整个模型设计过程代码是自动生成的，当然，一些复杂指标计算逻辑是通过代码片段的形式作为配置项提交。</p><p>代码的自动生成除了提升开发效率外，还带来额外的好处，因为计算代码是动态生成的，汇总表是否生成真正的物理表，对用户是透明的，平台可以根据成本、性能、效率等的考虑，来动态决定是构建物理表（也就是OLAP Cube），还是只是一个视图，背后是直接下发查询到事实明细表。</p><p>大家有兴趣可以了解下阿里的 <a href="https://www.aliyun.com/product/dataphin">Dataphin</a> 产品。</p><h4 id="代码的自动优化">代码的自动优化</h4><p>实时/离线计算引擎的不断发展演进，越来越多人肉在做的优化最佳实践会被集成到引擎里，做到在线识别、自动优化。</p><p>比如 Join 长尾（倾斜）问题，有一个优化方式是把热点数据提取出来，然后拆分任务，用大小表的 mapjoin 机制来提速；比如 count distinct 倾斜问题，可以基于热点字段做数据的二次分发，将查询拆成两层，内层子查询基于二次分发的数据做聚合，外层查询再按热点字段聚合。后者已经被 Flink <code>partialAgg</code> 机制所支持，开发人员使用普通 SQL 开发任务即可。相信这些有规可循的优化“套路“会越来越多地被集成到计算引擎中。</p><h3 id="三olap-cubes-终将衰落">三、OLAP Cubes 终将衰落</h3><p><a href="https://en.wikipedia.org/wiki/OLAP_cube">OLAP Cube</a> 又称 Data Cube，工程实践上是对（明细）数据表基于合适的维度组合（基于业务确定的维度组合或基于重要维度的笛卡尔积组合）做预先聚合计算，典型的计算机领域以空间换时间案例。</p><p>这种预计算模式，通过为下游应用提供稳定的查询性能，长久来促进了数据仓库的发展，我们通常说的集市层“百花齐放”，快速响应业务诉求，指的就是 OLAP Cubes 的建设。数据仓库建模理论，如 Kimball 的维度建模理论，本质上就是解决如何基于业务的分析诉求，科学的定义数据仓库中数据的组织方式，让数据开发工程师更好更容易的构建 OLAP Cubes。</p><p>技术的限制让这种模式存在并流行，这种模式反过来又在塑造数据团队的组织形式和职能，成为一种行业标准。做个假设，如果我们当前拥有极为充足的计算能力，很便宜的内存资源，还有能高效利用它们提供足够优秀查询性能的数据库，我们是否还需要花费大量人力基于明细数据去开发一个个应用层汇总表来解决多样的数据查询诉求？</p><h4 id="计算技术的成熟">计算技术的成熟</h4><p>第一个因素是摩尔定律，带来了计算和存储成本的不断降低，公有云的高速发展，按需购买，按量付费的模式，进一步降低了数据的存储和计算成本。</p><p>第二个因素是类MPP计算架构，列存储模式数据查询引擎的技术突破和成熟，同等资源下，能提供成倍甚至几十倍的查询性能提升。</p><p>从技术上来看，停止建设 OLAP Cubes，所有请求直连明细数据是可能的。但从业务上来看，所有的数据查询请求直连明细数据，存在两个潜在问题：1）查询请求过于复杂，不易理解，且容易出错；2）数仓汇总层会变得很薄，业务人员要从明细层取数，效率变低。</p><h4 id="数据应用的契机">数据应用的契机</h4><p>数据应用端的两个发展趋势一定程度上可以解决上述潜在问题。</p><p>第一个趋势是BI工具的演化，从提供优秀的报表制作及数据可视化能力，到兼具高级分析的”计算“能力。用户不再需要费脑力去思考如何写复杂的取数 SQL，而是通过 BI 工具的拖拽可视化，以及简单易用的计算字段配置来进行数据的分析和探索，如 YoY/MoM/DoD 对比、年/月/日汇总和趋势分析、字段级联组织和下钻分析等，都是通过系统配置自动支持，不用写 SQL。</p><p>第二个趋势是交互式/对话式查询在数据产品上的应用越来越多。这类数据产品模式的目标是提供更大的灵活性给用户，数据产品不仅仅只是看数，而是用户参与其中，定义自己的看数视角。以用户行为转化漏斗分析举例，常规数据产品会首先根据业务诉求定义转化过程重要节点，数据开发进行需求开发，然后通过数据的可视化展现服务用户。而交互式分析模式首先是对转化分析方式进行抽象：转化漏斗分析是对漏斗窗口期内，所有满足限制条件的用户行为，按既定步骤顺序的转化计算，以漏斗图的可视化形式展现。产品模块定义如下：1）漏斗名称设置组件，2）漏斗窗口周期设置组件，3）漏斗步骤设置模块，其中每项步骤包含用户行为选择和限制条件配置，4）漏斗图展现组件。至于对哪些行为做分析，是否需要对该行为再做条件筛选，关联多长时间的数据做时间序列追踪等，交由用户选择，即席查询。</p><p>概括来说，就是使用可视化分析工具替代取数 SQL 开发，产品化构建交互式分析场景替代集市主题表建设。</p><h2 id="数据开发从业者的3个核心能力">数据开发从业者的3个核心能力</h2><p>前面讲了数据开发技术的三个方向：1）流批一体成为主流开发模式，2）代码自动化技术走向成熟，3）OLAP Cubes 终将衰落。对于数据开发从业者而言，在技术的发展中，如何持续保持个人竞争力，我认为最重要的是如下三项能力。</p><p><strong>1、能深入理解你所服务的业务</strong></p><p>只有深入理解业务，才能真正知道当前业务处在什么阶段，碰到了什么问题，重点目标是什么。对应到企业的数据建设，一定要先解决“为什么”的问题，当前数仓服务的业务现状是什么，为了解决业务什么问题，期望达到什么目标，这些是无法靠技术自动化解决的。然后才是模型设计、实施落地。</p><p><strong>2、有把数据做深的能力</strong></p><p>数据会被用来搭建一个个分析报表，服务一个个数据产品，好像数据产生后，就和数据开发从业者无关了，以至于从业者很多自嘲是“人肉SQL机器”，是“数据搬运工”，也经常被合作方称做“ETL工程师”。把数据做深的能力是指生产数据之外，能持续去思考从这些数据里能获取什么，不管是通过数理统计还是机器学习，探索能否挖掘出推动业务增长的洞察，以及行动指引，是做“数据掘金者”。</p><p><strong>3、具备数据链路的全局观</strong></p><p>数据链路的全局观不仅仅是清楚整个数据架构是什么样子，熟悉数据是如何流转的，更是能做数据链路的全局优化。如整个数据链路的稳定性保障，数据资产的组织和管理机制设计，数据的全链路价值评估、成本治理，数据的质量管理及测试、监控机制的建设等。</p><section class="footnotes" role="doc-endnotes"><hr /><ol><li id="fn1" role="doc-endnote"><p>本文仅代表个人观点。<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;数据开发技术的3个方向&quot;&gt;数据开发技术的3个方向&lt;/h2&gt;
&lt;p&gt;未来数据开发技术方向，我认为有三个，首先是流批一体成为主流开发模式，其次是代码自动化技术走向成熟，第三是 OLAP Cubes 终将衰落。&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot; role=&quot;doc-noteref&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>2018年的两段记录</title>
    <link href="http://blog.zhengdong.me/2020/11/27/some-thoughts-in-2018/"/>
    <id>http://blog.zhengdong.me/2020/11/27/some-thoughts-in-2018/</id>
    <published>2020-11-27T15:40:45.000Z</published>
    <updated>2020-12-10T14:15:08.841Z</updated>
    
    <content type="html"><![CDATA[<p>整理写的东西，发现18年的这几天，写过这么两段话，当时在 Vegas 参加 AWS re:Invent。</p><p>第一段是关于在中国做 to B。</p><blockquote><p>中国做 to B 相比美国难做（重且周期长），是因为中国目前不是一个契约社会，是一个感情社会。很多客户在接受 B 端企业服务方面，还处于婴孩期，倔强任性，什么都要，无视契约，过度重视服务方表现出来的投入力度，即感情分。当然这些客户会随着发展成长起来，很多区域已经有这样的趋势。</p></blockquote><p>第二段是关于在集团公司里面搞阿米巴解决中后台价值难评估问题。</p><blockquote><p>做公司内部平台、中台等支撑业务会碰到很多问题，通过内部结算来体现价值，来确定投入规模，让财务、HR来横向协同、处理集团各事业部和平台中台部门的结算，是很难做好的。不懂技术，在冲突问题上，很难有大的规划，来定夺取舍。一个大公司，平台中台帮助业务做大，然后从大的业务获得更多的财务回报，来保障研发，从而让企业新增的小业务能够直接享受这些研发成果，助力新业务创新，这是平台中台的根本。如果业务做大了后，认为支出过高，便用安全、投入、需求响应等理由投入人力自研基础组件，去重复造轮子，企业的平台中台很难做好，毕竟出不了成绩，决策层会不认可。后果是小业务基本很难起来，最后可能变成了大业务有钱可以做内部创新，集团公司层面的业务创新却因为部门结算成本的问题越来越难做。解决集团公司平台中台技术协同问题有两个解，一个是横向技术管理，如公司确定CTO或者建设技术委员会，并且给予充分授权。二是打破集团公司内部的各个山头，让技术管理者能够跨部门，在业务、平台中台之间周期调派，让大家开放起来，而不是守着、扩充着自己的势力范围，生怕别人进来占了自己的自留地。</p></blockquote><p>两年过来，新的感想。第一段，不是契约社会只是一个角度，相比发达资本主义国家，我们的「商业社会」发展不充分，在很多领域没有形成完整的产业链、上下游意识，尤其在知识产品上。因而企业要么付费意愿弱，觉得这服务不值，要么宁愿一杆子捅到底都自己做，成本虽高但觉得安全。未来企业愿意购买服务来提升自己的商业效率一定是主流，比如充分竞争的零售行业企业付费意愿已经很强。看好 to B，尤其细分领域的 SaaS 服务。</p><p>第二段，财务算清是必要的，但目标是让前台业务的财务状况更清晰，看清业务才能更好决策发展方向。中后台完全按照内部结算来评估价值，可能带来的问题比解决的问题大很多。扪心问，一个企业什么时候会对中后台做强成本考核？是不是前台业务发展变缓甚至停滞的时候？回想起杰克韦尔奇在「商业的本质」一书里写的企业面对创伤如何补救的一条建议：</p><blockquote><p>摆脱困境的确并非易事，但如果留不住那些最优秀的人才，你永远不可能摆脱困境。因此，在公司面临困境之际，领导者不要在本能驱使下裁员降薪，而是要逆本能而动，采取一些鼓舞人心的举措，短期性的措施如涨薪，长期性的措施如根据其业绩表现给予更多的公司股份，也就是说要尽量想办法留住人才，而不是减少人才。</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;p&gt;整理写的东西，发现18年的这几天，写过这么两段话，当时在 Vegas 参加 AWS re:Invent。&lt;/p&gt;
&lt;p&gt;第一段是关于在中国做 to B。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>谈谈数据产品的商业化</title>
    <link href="http://blog.zhengdong.me/2019/01/16/data-product-creating-and-selling/"/>
    <id>http://blog.zhengdong.me/2019/01/16/data-product-creating-and-selling/</id>
    <published>2019-01-16T13:50:15.000Z</published>
    <updated>2020-12-02T16:12:21.653Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://blog.zhengdong.me/2019/01/07/speaking-of-a-data-team/">上篇</a>文章以一个数据团队发展的视角，总结了数据团队要做的事。当然，一些企业业务复杂起来之后，数据团队的职能也会发生分工，负责基础数据建设的团队，将数据集成、治理、资产管理、质量管控等工具及规范做掉，同时构建数据仓库的公共层并对上游业务以产品化和服务化的形式提供支持，现在比较热的「数据中台」概念就是指这块事情，和负责业务数据建设的团队，基于公共层数据，直接对接业务，让数据产品化、智能化，赋能业务发展。</p><h2 id="数据产品的四个层次">数据产品的四个层次</h2><p>这篇文章，主要想聊下数据产品，在我的理解里面，数据产品可以分为以下四个层次。</p><p>首先是通用的数据分析及可视化工具，能够连接各种数据，解决分析师、运营等岗位人员查看和分析业务数据的需求，如进行数据的多维分析，数据报告的制作等工作。</p><p>其次是场景化的数据分析产品，通用工具只解决了获取和分析数据的需求，但其实很难要求每个岗位都有好的数据分析能力，知道怎么用数据、怎么用好数据，知道数据的价值。场景化数据产品指的是用分析师沉淀下来的分析思路或分析模型来组织和展示数据，降低数据分析的门槛，提高数据化运营的效率。比如固化 ”AARRR“ 分析模型的用户分析和流量分析产品、活动大促的专题分析产品，面向业务场景的实时数据大屏产品等。</p><p>第三是数据嵌入业务系统的能力和服务，让业务系统可以很方便的连接和使用数据，迅速实现业务数据化。拿精准营销或用户运营系统举例，业务研发团队只需要做好系统流程相关的开发，如营销渠道管理、人群筛选方式和逻辑、触达用户途径和技术对接等，而基于用户特征及行为的分群、画像能力、高性能的数据检索、聚合能力，则由这类数据产品来统一以服务化或内嵌的方式提供。</p><p>第四是数据决策类产品，集成业务知识和数据智能的技术及能力，除了用数据描述业务现状，诊断业务问题外，能进行业务预测和决策支持，比如基于趋势预测、行业洞察直接服务高层管理者的决策。</p><h2 id="数据产品的商业化">数据产品的商业化</h2><p>第一二层次的数据产品，要么通用，要么聚焦场景，通过泛化形成与企业业务细节无关的通用数据产品是可行的，行业里有很多企业在做，也早已被证明是能赋能企业，促进数据化决策效率，降低数据化运营的门槛和成本。</p><p>接下来谈谈在商业化赋能企业的历程中，我的一些收获。</p><h3 id="三个取舍">三个取舍</h3><p>我们从服务自身业务出发，对数据产品提的要求是要足够灵活是真正的多维分析，要有强大的探索分析和可视化能力，还要具备非常好的用户体验。“人人都是数据分析师”的口号听起来很好。</p><p>但真正作为软件服务提供商，面对外部企业，碰到了很多问题。</p><h4 id="客户和用户">客户和用户</h4><p>服务自身业务的时候，我们面对的是一个个用户，所以虽然是 to B 产品，但可以用 to C 的方式去触达和运营产品。</p><p>面对外部企业，很多时候，买单决策方（即客户），和产品使用方（即用户），是不一样的，彼此的目的、期望和利益也不同。比如客户是希望购买了产品后，能解决企业什么问题，如何在众多厂商中选择最佳的供应商，企业采购的预算如何等，用户则往往关注产品是否能解决工作中的问题，是否符合自己以往的使用习惯，是否稳定，是否支持某某功能等。</p><p>一味强调产品的功能往往是无法吸引客户的，也会陷入厂商之间比 feature 比价格这种低质的竞争中。基于产品解决客户实际问题和痛点的方案输出，会非常重要，也是产品要去重视和沉淀的能力。当然，落地到企业里真正被用起来，产生多大的价值是要依赖用户的，无视用户需求也很难成功。</p><h4 id="阅览者和编辑者">阅览者和编辑者</h4><p>用户用产品主要完成两件事，通过分析探索，形成决策依据，或者通过数据可视化，制作数据报告。</p><p>数据产品的核心能力往往会倾向在数据分析和可视化能力方面，让用户可以深度探索数据，便捷做可视化报告。可以称这些用户为编辑者。</p><p>数据报告要被大量消费才有价值，看数据报告的人（称之为阅览者），和做数据报告的人即编辑者，很多时候是不同的，尤其在岗位职能比较细化的中大型企业里，阅览者往往多过做编辑者，并且很多阅览者的决策层级也高于编辑者。</p><p>不能忽视阅览者的诉求，比如怎么解读数据报告，怎么管理数据报告，对数据有疑惑该找谁，如何分享数据报告，如何批注、评论等。</p><h4 id="功能强大和上手门槛">功能强大和上手门槛</h4><p>有些数据产品，功能多，给高级用户（Power User）很大的可操作能力，但往往会有高上手门槛的副作用。一个没有经验的用户，一开始使用产生的价值可能只能达到50分，但经过一段时间的学习，有些用户是可以做到120分的。</p><p>还有些数据产品通过一定的功能削减，来降低上手门槛，让一个没有经验的用户就可以做到80分，但高级用户的能力会受限，因为产品的能力最多也就90分。</p><p>如何权衡强大的功能和很低的上手门槛，解决新手和高手之间不同的诉求，是值得深入思考的。</p><h3 id="两种模式">两种模式</h3><h4 id="私有化部署on-premises">私有化部署（On-premises）</h4><p>数据产品一般是要连接或者采集企业的各种数据，很多企业不希望自己的数据出去，要求采用传统的私有化部署模式。</p><p>这种模式，对数据产品而言，在产品的基本能力之外，要有能力连接企业的其他系统，比如企业的用户账号系统，要有能力嵌入到企业的内部系统，如企业的门户系统（Portal）。总结起来就是<strong>开放和集成</strong>的能力。</p><p>另外要降低后期运维服务的人力成本，产品要足够稳定，要能自动化部署，要有完善的运维工具来提高问题定位和解决效率，总结起来就是<strong>稳定和运维</strong>的能力。</p><h4 id="云服务saas">云服务（SaaS）</h4><p>云服务让数据产品可以以很低的成本服务企业客户，按服务、按量收费的模式也能大大降低企业使用产品的开销。</p><p>产品的更新或问题解决能以更快的节奏给到企业客户，私有化部署可能带来的碎片版本，隔几年憋大版本重新收费的痛苦变得不复存在。</p><p>这种模式会对数据产品带来哪些挑战？</p><p>首先是生态的能力，这类企业的业务很大可能也已经在云上，那么如何去连接云上数据源的数据，甚至连接企业在云上其他 SaaS 服务的数据，会是不可或缺的能力。</p><p>其次，云服务面对的客户会更广，分类会更细，可能存在大量的小微企业。那么产品在使用门槛方面需要尽可能的降低。输出业务场景化的分析模版、或者提供基于领域的解决方案，会是一个途径。</p><p>第三，云服务的另一个优势在于可以平台化，虽然逻辑上客户之间是不可见的，但物理上是在同一个平台的，那么是否让不同客户能够分享甚至交换数据？是否要扩充、集成其他服务，比如提供云上数仓服务，甚至大数据基础服务？需要做抉择。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;http://blog.zhengdong.me/2019/01/07/speaking-of-a-data-team/&quot;&gt;上篇&lt;/a&gt;文章以一个数据团队发展的视角，总结了数据团队要做的事。当然，一些企业业务复杂起来之后，数据团队的职能也会发生分工，负责基础数据建设的团队，将数据集成、治理、资产管理、质量管控等工具及规范做掉，同时构建数据仓库的公共层并对上游业务以产品化和服务化的形式提供支持，现在比较热的「数据中台」概念就是指这块事情，和负责业务数据建设的团队，基于公共层数据，直接对接业务，让数据产品化、智能化，赋能业务发展。&lt;/p&gt;
&lt;h2 id=&quot;数据产品的四个层次&quot;&gt;数据产品的四个层次&lt;/h2&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>从一个数据团队的说起</title>
    <link href="http://blog.zhengdong.me/2019/01/07/speaking-of-a-data-team/"/>
    <id>http://blog.zhengdong.me/2019/01/07/speaking-of-a-data-team/</id>
    <published>2019-01-07T13:42:33.000Z</published>
    <updated>2020-11-28T10:52:32.836Z</updated>
    
    <content type="html"><![CDATA[<p>大数据时代，在有庞大自有数据的企业，作为一个承担数据体系建设责任的数据团队要从哪些事情开始做起？</p><p>一开始，数据的需求很多都是企业的领导者要快速了解公司的业务情况，比如销售、财务、研发环节的一些统计指标。</p><p>于是数据团队开始熟悉企业的各种数据，把各种不同数据源的数据汇集到大数据技术栈：把业务数据库的数据同步出来，把在线系统的日志收集起来，把用户在产品各端的行为记录采集起来。接着，基于大数据技术栈，针对性做数据清洗、数据统计，然后将数据展现出来给领导者。</p><p>几个几十个指标这么做问题不大，但数据需求很快膨胀了起来，指标需求增长到好几百，数据团队开始疲于奔命响应业务方繁杂多变的需求。每个指标都要从原始数据算起，重复工作很多，加上都是独立做计算，同类指标口径歧义的问题也越来越严重。同时，快速增长的计算任务，带来任务产出稳定性、及时性的大幅下降。数据团队同时面临开发人力的不足、数据产出的不稳定以及业务方的频繁不满。</p><p>团队思考再三，为了解决困境，决定从以下两件事情做起。</p><p>第一是建立数据研发的系统，解决任务开发、调度、运维、质量保障方面的问题，把数据开发和数据产出管起来。</p><p>第二是建立企业的数据仓库，通过对企业业务和数据的调研、梳理，将企业各个业务领域的数据规整起来。通过定义一系列设计规范，完成数仓模型的设计，接着，基于数仓模型实施数据开发。</p><p><strong>数据仓库建设，是一个企业数据体系建设过程中很关键的一项工作，也是工作量巨大的一项工作。</strong></p><p>数据仓库建设过程中，产生的标准规范、表结构、任务依赖、存储计算资源、业务用途等信息，需要统一管理起来，如通过数据地图，让数据可见，而不是在一行行代码一个个计算任务里；通过血缘追溯，管控数据的依赖关系，数据的质量保障和问题跟踪能有据可依；数据还可以被从企业数据资产的角度来看待，来盘点。</p><p>有了数据仓库，有了不断丰满的业务指标体系，数据团队的工作变得规范、高效起来，团队也步入正轨。这个时候，新的问题出现了，企业业务高层领导者开始质疑：数据团队这么多人，做了这么久，怎么没看到什么产出，没看到对业务有什么实在价值？就做做数据报表需要这么多人？</p><p><strong>数据团队的价值在哪里，是整个团队面临的又一个大问题。</strong></p><p>团队开始思考，如何让数据产生价值。有几个想法大家越来越确定，</p><p>数据要产生价值，不是收集尽可能多的数据然后躺在那里，也不是开发尽可能多的指标然后展示在那里，而是要被用起来，被“活”用起来。</p><p>我们不能假定用户都会知道数据的好处，而是要降低用户使用数据的门槛，让用户知道怎么用数据、怎么用好数据，知道数据的价值。</p><p>一个个数据报告是静态的，但分析的理念和框架是动态的，是可行动的，通过提炼分析框架进行数据泛化形成数据产品，如 “AARRR” 模型的用户分析产品，如针对大促的全流程电商运营分析产品，才能让数据被真正用起来。</p><p>于是团队开始基于业务现状、目标来研发数据分析产品。在过程中，积累了不少心得：</p><ul><li>数据产品要聚焦业务场景</li><li>数据产品也要重视用户体验</li><li>数据产品要打通分析、行动，形成闭环</li></ul><p>有了数据产品，用户可以以很低的门槛使用数据，用数据做决策。接下来，数据团队面临的挑战又是什么呢？</p><p>挖掘数据价值，让数据不仅能通过分析框架来指导业务，还可以直接赋能业务，让基于数据洞察做业务创新变得可能。</p><p>团队通过用户画像做精准营销，做千人千面，通过基于行为的用户分类做用户促活、做流失用户挽回，通过销量预测做商品补货、做商品调拨 ……</p><p><strong>从数据分析到数据洞察，从业务描述、业务诊断到业务预测、决策支持。</strong></p><p>最后，总结下数据团队都做了哪些事情：</p><ul><li>数据采集和集成</li><li>数仓规划和建模开发</li><li>数据治理和数据资产管理</li><li>数据产品和服务</li><li>数据挖掘和算法赋能</li></ul><hr /><p>时下，很多大数据厂商不管是提供基础设施的，还是提供 PaaS 平台的，又或是提供数据应用的，都喜欢说自己是大数据整体解决方案提供商。回看这个数据团队所做的事情，这些厂商要做到什么程度才能称之为整体解决方案？才能从工具层面解放这个数据团队，让其聚焦到业务上，做产出最大的事情？</p><p>最基础的是覆盖技术栈广、性能高、且稳定的数据基础设施，满足数据离线/实时计算、存储、查询等需求。</p><p>然后是完备的数据开发工具支持，高效解决数据集成、数据开发、任务依赖管理等工作，以及完善的离线/实时任务监控、运维工具。</p><p>第三，配合数据开发工具，要能形成整合统一的元数据中心，解决数据治理问题，如数据的全链路血缘追溯、数据质量的监控保障等。另外，针对数据仓库建设，要从工具和标准规范层面打通数仓规划、模型设计、开发、测试等流程，降低数仓建设门槛。最后，能提供数据管理的门户，让从业务视角去管理企业的数据资产变得可能。</p><p>第四是通用和场景化的数据分析产品及服务，如可视化BI产品、应用/用户分析产品、营销监测产品、A/B试验产品、数据查询服务、用户画像存储和查询服务等，让数据团队不用花费大量时间去造轮子。</p><p>第五是机器学习算法建模、调试等工具支持，降低算法模型开发的上手难度，驱动数据团队基于数据智能的业务创新实践。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;大数据时代，在有庞大自有数据的企业，作为一个承担数据体系建设责任的数据团队要从哪些事情开始做起？&lt;/p&gt;
&lt;p&gt;一开始，数据的需求很多都是企业的领导者要快速了解公司的业务情况，比如销售、财务、研发环节的一些统计指标。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>关于大数据变现的一些思考（下）</title>
    <link href="http://blog.zhengdong.me/2018/10/28/ways-to-profit-from-the-bigdata-on-data/"/>
    <id>http://blog.zhengdong.me/2018/10/28/ways-to-profit-from-the-bigdata-on-data/</id>
    <published>2018-10-28T12:30:33.000Z</published>
    <updated>2020-11-28T10:52:32.834Z</updated>
    
    <content type="html"><![CDATA[<p>上篇从数据技术的角度谈了自己对大数据变现的一些思考，这篇继续，从企业的数据资产角度入手，谈谈变现的方式。<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p><p>数据资产变现是指企业通过自身拥有的数据进行的商业化变现。我始终认为数据已经是新的生产力，企业应该把最大的资源、最全的数据，首先用于自身，让数据驱动业务发展。接下来才是去想如何做商业化变现，不能本末倒置，当然，核心业务就是数据变现的企业另说。</p><h2 id="在线广告">在线广告</h2><p>从门户网站开始，在线广告模式的变现就是很多互联网公司收入的主要来源，现如今，全球和国内广告收入的一二名都是互联网公司。</p><p>最常见有两种广告方式，一个是品牌（合约）广告，一个是效果广告。</p><p>品牌广告是从传统的线下广告、电视广告发展而来的，按曝光量来计费，客户的核心诉求是在固定投入的前提下最大化对目标用户的曝光量。效果广告则是按效果计费，客户的核心诉求是以较低的转化成本达成较高的转化规模。比如我们经常看到的视频贴片、门户banner、App开屏等广告多属于品牌广告方式，而通用搜索、电商搜索、信息流等广告采用效果广告方式居多，当然也有融合的，比如信息流广告也存在一部分品牌广告。</p><p>有流量就可以做广告，但品牌广告方式下，如何在满足曝光量的同时，定位到客户的目标用户，最大化利用自身流量，效果广告方式下，如何在提高客户转化效果的同时，最大化自身广告收入，都需要数据来支撑。</p><p>这里引申出广告变现模式的两大前提：用户体量和清晰且有价值的目标人群。</p><p>要做这个生意之前，首先是要对自己的用户体量有认识，体量太小自己做广告平台投入产出比太低。其次是要能对自己的用户有深度洞察，数据不好连自己用户的画像都做的不准，这门生意也做不好，比如明明自己的用户大多是中小学生，却接了汽车厂商的广告，自己的用户大多是中年男性，却接了麦当劳的广告。</p><p>这也是为什么在线广告市场可以算是互联网大厂的自留地，既有大的用户量，又有沉淀下来精准的用户数据。那如果有精准、全面的数据，却没有流量，还有什么方式做变现？</p><h2 id="营销优化">营销优化</h2><p>广告营销市场的规模非常大，企业在营销方面的预算，往往比采购技术产品大很多，这也是很多厂商依仗着数据往营销优化市场去挤的原因，能帮客户提升营销效果、降低营销成本，是实打实能打动客户的，只要效果够好，议价能力会很强。</p><p>前面数据技术变现部分提到过一种“羊毛出在猪身上狗来买单“的变现模式，很多就发生在数据驱动营销优化这门生意上。</p><blockquote><p>通过免费模式吸引客户，在服务客户的同时，沉淀客户数据并整合，最后，通过这些数据，从别的客户那里赚钱。</p></blockquote><p>业内有几种做法：</p><p>基于自身拥有的用户及行为数据，建立用户ID和标签体系，通过输出用户画像信息，以三方 DMP 的名义接入 DSP，来获取分成，一些中小互联网企业会这么干。</p><p>还有些企业直接贡献自己的数据，和广告主合作建设广告主的一方 DMP 来变现，甚至有的还会帮客户方做实施，除了贡献数据外，帮企业建立一整套定制的如站内推荐、用户画像、商业分析、营销效果评估、可视化报告等系统，不过这个更多属于数据技术变现范畴了。通常这些企业的数据质量不够好，价值不大，才愿意直接贡献出来。</p><p>第三种方式，对一些比较在意数据安全，不愿意直接对外输出的企业，可以采用碰数据的形式：通过数据开放服务，广告主上传用户列表，企业在自己的 DMP 中进行匹配，输出用户所对应的标签信息，来进行数据变现。</p><p>当然，这种“开放”做多了，数据等于还是出去了，所以很多企业，会把后面的用户触达也给做了，这样广告主是拿不到用户信息的，只能通过用户标签选择用户群，或是通过 lookalike 做用户放大，不少中大型互联网企业提供这类服务。</p><p>这个模式能做好，自身数据的全面和准确很重要，比如碰数据的时候，能匹配到广告主给的用户的比例太小，匹配到的用户标签不准、标签的商业价值很低等，是做不出好的效果的，也做不长久。</p><p>虽然大数据技术大大促进了如今营销业的发展， 但它其实是个老产业，盘子大，做的企业很多且鱼龙混杂，水挺深。</p><h2 id="saas-的另一种出路">SaaS 的另一种出路</h2><p>从厂商的角度看，产品通过 SaaS 的形式提供服务，能大规模部署，高效运维，也能持续迭代升级，迅速修复问题，还能防止买断式模式带来的尾款、服务费等不能收回的风险。对客户而言，在线试用、在线支付、按使用时长或资源付费等能大大降低接入和使用成本。</p><p>但从数据的角度看，产品 SaaS 化还存在另一种潜力，能大大提升整个产品的价值和变现能力，那就是：<strong>通过 SaaS 服务触达上下游，沉淀全网数据，再通过数据服务来变现。</strong></p><p>数据变现部分提到过数据产品免费提供服务、通过数据变现的例子：</p><blockquote><p>拿提供移动应用分析产品的厂商举例，一开始免费接入客户，不断提升产品的体验和能力，强化在市场上的竞争优势，接下来，通过数据榜单、行业会议、线上线下活动等市场行为不断强化客户的认知，比如覆盖多少应用、多少设备、多少行业、数据有多准多全等，最后，依仗着全网的数据，在金融、房地产、零售等行业赚钱，比如提供基于精准用户定向的跨应用营销，个人信用、收入、兴趣等的人群洞察，地理位置消费能力热图，手机品牌保有量地区分布、人群品牌偏好等服务。</p></blockquote><p>其实不仅是数据类产品，这也可以是其它各类 SaaS 产品的另一种出路。比如面向个人的线上免费记账产品，可以包装个性化理财服务进行数据变现；连接用户和商户的客服产品，可以包装精准用户定向的触达服务进行数据变现。</p><p>当然，所有这些的前提是，得能连接上下游，得有全网或全行业的用户覆盖。</p><h2 id="垄断数据的魅力">垄断数据的魅力</h2><p>这里首先当提电商平台型企业都有的生意参谋类产品。购买了这个数据产品，商户除了能看到自己商品的流量、交易、财务、物流等分析报告外，还能和同品类、同行业的数据进行对比，能获取自己用户的画像数据，更好的提升服务和制定决策，能 …</p><p>而这些东西，都是要付费的，往往面对的还是按功能区分的阶梯定价模式。通过对垄断数据进行产品化，进而定价售卖，是门很赚钱的生意，关键是除了所在平台外，没有谁能提供这些服务。而之所以当年能产生这个变现模式，就是因为电商平台在自身业务的经营中，数据变得越来越重要，并且通过数据产品化产生了很大价值，对外输出是顺便的事，就赚了大钱。</p><p>好做的生意总是门槛高的，首先得有<strong>全面的、可信度高的、聚焦行业的垄断性数据</strong>。这种模式，大家还能想到其他厂商么？</p><h2 id="数据交易">数据交易</h2><p>数据交易，主要是技术投入少、水平不高或商业化能力受限，但有大量数据（全网或覆盖单个行业）的企业，常采用的数据变现方式。表现出来的就是一锤子买卖和白菜价。</p><p>当然，有些“垄断”数据的企业，拿国内运营商举个例子，可能会通过差异化的数据包来解决一锤子买卖的问题，如对特定地区用户群体的出行轨迹、主流 App 的使用频次分布等数据单独售卖，也会寻求和其他企业通过数据层面的合作去变现，至于做了什么效果如何，就不知道了。反正经常经历或听说某些运营商劫持用户流量投广告赚钱，真是没出息。</p><p>其实对于运营商而言，抓着全网的数据，理应能做垄断数据的生意，也许是因为隐私保护政策的原因做不起来吧，但我想更大的原因在于前面一节我提到过的：</p><blockquote><p>而之所以当年能产生这个变现模式，就是因为电商平台在自身业务的经营中，数据变得越来越重要，并且通过数据产品化产生了很大价值 。。。</p></blockquote><p>数据交易之外，数据报告也是一种变现方式。拥有的数据企业可以通过自己做，或和行业咨询公司合作，由专业人士对数据加以分析、挖掘、<del>美化</del>，形成特定领域的行业分析、市场研究、销售状况等咨询报告面向社会销售加以变现，效果如何很大程度取决于出品方的市场口碑和公信力。</p><h2 id="数据合作">数据合作</h2><p>企业或多或少都拥有数据，一个企业的数据可能会对其他企业的业务发展有用，也可能觉得其他企业的某些数据对自己的业务会有帮助，这个时候，数据合作就成为一个诉求，也是数据作为一种资产，必然会面临的。</p><p>数据合作，首先要保护企业的数据知识产权，保护用户和企业的隐私，其次，需要能融合、匹配多个企业的数据，否则合作也就失去了价值。</p><p>业内有种合作模式，通过中立第三方实体作为数据合作平台，当然，这个第三方需要由法律协议来约束，也要接受审计。</p><p>首先企业双方将数据上传至第三方实体，上传过程中关键数据字段可以进行脱敏。接下来彼此的研发人员就可以通过数据合作平台提供的工具来进行数据、算法等的开发。开发人员查询、调试过程的数据都是经过限制条数、动态采样以及脱敏的，合作平台会从技术层面来保证安全。当基于企业需求的算法模型开发完成上线后，才会真正运行在平台的全量数据上，最终企业双方都获取满足自身需求的数据产出物，而双方的明细数据在整个合作过程都是彼此保密的。</p><p>这种合作模式下，保险公司可以和车载系统服务商进行数据合作，通过对用户的行车数据进行分析，通过算法模型做到千人千面的差异化车险定价；信用借贷公司可以和电商企业进行数据合作，实现差异化授信额度的制定和风险控制。</p><p>数据合作模式要跑通，需要具备公信力的第三方，合作平台也要能从技术上保证彼此明细数据的不可见，还要提供便捷的开发套件供数据开发，算法调试使用，有一定的技术门槛。最后，数据合作模式下，如何进行定价也是一个问题，毕竟在最终验证业务价值前，企业双方对彼此数据价值的认知可能是不一致的。</p><h2 id="写在最后">写在最后</h2><p>数据资产变现部分写到的几种方式，是目前比较多在用的，也或多或少验证可行。数据资产变现从大的角度来说还有很多，比如在医疗领域给医学发展带来很多可能，助力金融等行业的变革，推动人机对话，自动驾驶，智能家居等技术的从无到有到大规模普及等等。</p><section class="footnotes" role="doc-endnotes"><hr /><ol><li id="fn1" role="doc-endnote"><p>本文仅代表个人观点。<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">&lt;p&gt;上篇从数据技术的角度谈了自己对大数据变现的一些思考，这篇继续，从企业的数据资产角度入手，谈谈变现的方式。&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot; role=&quot;doc-noteref&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;数据资产变现是指企业通过自身拥有的数据进行的商业化变现。我始终认为数据已经是新的生产力，企业应该把最大的资源、最全的数据，首先用于自身，让数据驱动业务发展。接下来才是去想如何做商业化变现，不能本末倒置，当然，核心业务就是数据变现的企业另说。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>关于大数据变现的一些思考（上）</title>
    <link href="http://blog.zhengdong.me/2018/10/27/ways-to-profit-from-the-bigdata-on-tech/"/>
    <id>http://blog.zhengdong.me/2018/10/27/ways-to-profit-from-the-bigdata-on-tech/</id>
    <published>2018-10-27T12:55:57.000Z</published>
    <updated>2020-11-28T10:52:32.834Z</updated>
    
    <content type="html"><![CDATA[<p>年初（2018）接受 DTalk 社区访谈 ，对“大多数企业怎样把大数据落地变现？”这个问题，我当时是这么回答的：</p><blockquote><p>我理解的大数据落地变现有两大模式，一种是基于大数据技术，另一种是基于已有的数据资产。大体有如下几种方式：</p><ol type="1"><li>输出平台型技术能力，通过给企业建设大数据平台来变现；</li><li>输出大数据处理技术和应用产品，比如把企业内部的BI、应用/用户分析、营销监测、数仓应用等产品进行商业化输出或者通过数据建模咨询和实施来变现。</li><li>基于数据的闭环服务变现，如营销方向的广告精准推送、金融领域的风控服务等。</li><li>咨询类的数据报告，针对不同领域提供对客户有价值的分析及数据报告等。</li><li>数据交易。</li></ol></blockquote><p>最近，在数据技术变现和数据资产变现这两个方向，我又深入梳理了自己的认识，并将我的思考写了出来，这篇是数据技术变现部分。<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p><h2 id="从-bi-产品开始">从 BI 产品开始</h2><p>BI 产品在大数据这个概念出来之前，已经是蛮成熟的品类。简单说来，就是能连接和有效整合企业内部各个数据源的数据，通过强大灵活的分析能力，以丰富的可视化形式来展现数据，帮助企业进行业务经营决策。</p><p>之前的主流厂商主要是国外的大型企业服务公司，产品往往会比较重，也会和自身的其他产品，如 OLAP 工具、CRM 等产品整合，整体使用门槛高，且需要不小的团队来维护。后来，很多新兴企业用敏捷 BI 这个品类，大大降低了产品的接入成本和使用难度，价格也大幅度低于以前，市场份额越来越大。因为产品比较通用，并且内部业务都会有需求，这也成为一些互联网企业数据技术团队内外输出数据产品的第一站。</p><p>下面几种商业模式使用的比较多：</p><p>第一种是 SaaS 模式，这个模式很轻，可以以较低的成本服务大量客户，所以收费比较低，一般按年和账号数来计费。但因为要连接企业的数据，尤其大企业，往往不能接受这个模式，不过公有云厂商提供这个服务反倒有很大优势， 因为企业用户的数据已经在云上。</p><p>接下来是私有化模式，产品部署到企业自己的环境中，说到底就是卖软件，硬件是客户的。有两种收费模式，买断制和订阅制。从现在软件售卖的市场看，订阅制是趋势。买断制起始价格高，但要生意持续，往往需要周期性憋大招升级大版本让客户升级，再次售卖。而订阅制价格相对低一些，对企业来说，因为按年持续有收入，业务会更健康，也不用花力气做大版本创收，持续改进产品就好，另外，还能大大减少维护多个遗留版本带来的研发投入。</p><p>最后一种是项目制，不仅仅是卖软件产品，而是解决企业客户的问题，比如报表体系建设、业务数据门户等，除了涉及通用产品外，还可能会有数据 ETL、数仓建模、报表实施等交付工作。</p><p>BI 产品商业化，虽然属于通过数据技术变现，但 BI 产品和业务离得很近，客户选择时，往往考虑业务多于技术，如何解决不同行业客户多样的问题，还能坚守产品的通用性，不陷入企业定制开发的漩涡；如何在价格透明甚至低价格战的竞争格局中去占领市场份额，去盈利，会是比较重要需要解决的问题。</p><h2 id="数据基础设施">数据基础设施</h2><p>数据基础设施，一开始是从解决业务问题的 OLTP 数据库开始，随着数据越来越被重视，数据仓库的概念被提了出来。传统的数据库厂商在 OLTP 产品之外，投入很多资源在 OLAP 产品的研发、推广上，同时，市场出现了不少 MPP 厂商，也占据了一席之地。然后大数据时代来了，随着大数据的概念、实践从互联网公司走向各个行业，大数据平台这个品类开始逐步替代传统数据库、MPP 厂商们。</p><p>大数据平台通常提供如下能力：</p><ul><li>数据集成 提供数据采集、传输/同步能力，能将各个环节、数据源的数据同步到大数据平台，统一存储。</li><li>开发平台 提供统一的开发环境，满足开发人员进行数据任务的开发、测试及运维等工作。</li><li>任务调度 提供任务托管和调度能力、并通过计算引擎完成任务的执行，大数据栈的很多技术会包含在这里。</li><li>数据管理 提供元数据管理、存储和成本管理、数据质量管理等能力。</li></ul><p>现在机器学习的概念很火，很多大数据平台也提供了机器学习方面的开发、建模、计算等支持。</p><p>公有云厂商和企业服务厂商采用了两种不同的商业模式：</p><p>第一种是 EMR 服务，公有云厂商大多都会提供，也是云里比较赚钱的业务。这个模式下，用户不需要去操心如何去选型、准备机器、搭建部署整个大数据平台，直接使用厂商提供的 EMR 集群，按需创建即可，计算结束可以很方便的释放，带来成本的降低。这个模式有很大的前提，就是企业的数据已经在云上，即通过公有云的基础设施服务来带大数据基础设施服务的售卖。</p><p>第二种是私有化部署模式，目前市面上的主流厂商大多都支持这个模式，包括很多公有云服务厂商，面对的客户主要是中大型企业。从竞争来看，国内外厂商很多，竞争比较激烈，不过这种模式一两家厂商很难做到垄断，而且不同厂商对市场、目标客户也或多或少有不同的定位，还在前赴后继的进入。</p><p>现在也有一些厂商在垂直领域，比如数据集成、数据仓库等方面提供服务，但能看到，很多做着做着，平台的一些能力都会逐渐加上来，变成大数据全栈服务提供商。这引申出来第一个问题，就是厂商同质化严重，因为基础组件大多基于开源，大家都是站在巨人的肩膀上。</p><p>另外，私有化模式的采购周期往往很长，因为涉及比较大的采购费用，厂商在平台功能、服务能力、性能指标、客户案例等方面，会面对客户很多挑战。</p><p>还有就是 lock in 的问题，大型的交付往往会对客户形成一定的 lock in，即一旦企业客户的业务在平台跑起来，换厂商的迁移成本非常高，所以，通过对既有客户的服务，挖掘客户更多需求，赚更多收入，是非常重要的。对客户而言，也会在意这个问题，这个时候，自研或基于开源的选择会对客户产生影响，基于开源，即使厂商提供不了服务，客户自己也能比较容易招人，继续维护，不像自研的，离了厂商什么都做不了，会成为一种有竞争优势的销售说辞。</p><p>第三种是解决方案模式，可以是技术咨询、也可以是工具+技术咨询、工具+项目实施、或是综合的行业解决方案，比如金融、政务、工业等的大数据解决方案。这块后面还会提到。</p><h2 id="数据应用产品">数据应用产品</h2><p>还在 web 主导的互联网时代，专注流量分析的 web analytics 产品已经很多，往往还是免费模式。移动互联网起来之后，市场上出现不少移动应用分析产品，以开发者工具的定位免费提供服务，很快发展了起来，除此之外，还有营销和广告监测、用户行为分析、A/B 测试、用户画像等解决特定领域问题的数据产品。</p><p>数据应用产品是指以标准产品的形式，利用数据，去解决客户垂直领域或特定场景下的问题。比如用户行为分析产品，就将数据采集、传输、计算、展现等工作打包起来，作为黑盒，客户不再需要关心技术细节，接入后，只需要看数据做决策就好。</p><p>这些产品大多是通过 SaaS 模式来提供服务，有以下几种变现方式：</p><p>第一种是免费模式，通过咨询服务变现。早先很多做移动分析的 SaaS 厂商，会提供基于客户数据的数据挖掘、用户画像等付费服务。</p><p>第二种是直接收费模式。一般按照数据量或账号数收费，还有按照功能集合进行分层定价。当然，这个模式的厂商也在寻求咨询服务方面的收入，比如搭配着产品，去做时下很火的增长黑客培训，或是直接帮助客户做业务增长的咨询和落地服务。</p><p>最后是“羊毛出在猪身上狗来买单“模式。通过免费模式吸引客户，在服务客户的同时，沉淀客户数据并整合，最后，通过这些数据，从别的客户那里赚钱。拿提供移动应用分析产品的厂商举例，一开始免费接入客户，不断提升产品的体验和能力，强化在市场上的竞争优势，接下来，通过数据榜单、行业会议、线上线下活动等市场行为不断强化客户的认知，比如覆盖多少应用、多少设备、多少行业、数据有多准多全等，最后，依仗着全网的数据，在金融、房地产、零售等行业赚钱，比如提供基于精准用户定向的跨应用营销，个人信用、收入、兴趣等的人群洞察，地理位置消费能力热图，手机品牌保有量地区分布、人群品牌偏好等服务。</p><p>数据应用产品的商业化过程会碰到些问题，一个是数据安全问题，很多客户尤其是大的企业不愿意数据在云上，导致这些产品的客户群往往是初创或中小企业，进入大企业很难。一些厂商会提供客户侧私有化部署的支持来吸引大企业，和前面提到的 BI 产品私有化模式一样，也会碰到部署实施、售后服务、客户定制需求等问题。</p><p>另一个问题是采购 vs. 自研的矛盾。拿用户行为分析产品举例，初创企业一开始还着立于验证商业模式，采用第三方云服务阻力不大，但随着业务的增长，尤其当数据被重视起来后，需要去建设自己企业的数据体系，第三方服务经常会被企业自研的产品所取代。放眼看过去，几乎每一个中大型互联网企业，都有自研的数据埋点、流量分析、用户分析、营销分析等产品，一些大企业内部还不止一套，可见自研的吸引有多大。</p><h2 id="大数据解决方案">大数据解决方案</h2><p>在 BI 产品部分，提到过项目制的模式，BI 产品在大数据概念流行前已经很成熟，很多企业已经被教育的认为合作模式就应该是这个样子的。所以很多时候，会在产品的基础上，根据客户的需求，把业务咨询、数据体系建设、数仓建模、报表实施等工作也做掉，以项目的方式交付给客户。</p><p>对于很多企业客户而言，尤其是传统行业的客户，采购厂商的数据产品是为了解决问题，它不希望面对一个个产品或是产品功能集合来选择，而是会看有没有同行业的企业在用，是怎么用的，对企业面临的需求有没有成熟的解决方案等。</p><p>业务咨询和数据运营指导、数据体系建设和数仓建模、定制化数据产品等企业服务，是很多解决方案提供商做了很久的生意。随着大数据、AI 的流行，企业的需求也越来越多，比如推荐系统、机器学习算法模型、用户画像体系等的建设。以上种种需求，是很多大数据平台通过解决方案在企业落地的切入点。</p><p>时下有两个大的概念：</p><p>一是企业的数据中台建设，主要是互联网企业在推，用互联网企业建设自己全域数据中台的理念、经验，去帮助企业建设自己的数据中台。二是行业的大数据解决方案，比如智慧城市、政务大数据、工业大数据、金融大数据等。这两块可以打包进去很多东西，能大大提高整个项目的报价，以及有比较大的持续合作的可能，客户价值更高。</p><p>做解决方案类项目实施虽然能提高客单价，但很重，而且这个行业一直是企业服务巨头的自留地（有些项目厂商驻场在一家银行能做十多年持续赚客户钱）。大数据技术的发展给了互联网厂商进入这个市场的机会，从技术积累到品牌形象都有竞争优势，不输传统企业服务厂商。当然，互联网公司深深的 to C 基因，一开始内心往往是排斥的，要给每个客户做，还要理解客户业务，完全不能 scale 呀。但慢慢大家会认识到， 在大数据企业服务市场，做标杆大客，沉淀解决方案，然后行业内复制也是一种有效的增长模式。</p><p>是不是做大数据解决方案就只是为了提高客单价，从单个客户赚更多钱？</p><p>我认为不全是，从客户角度上来看，大数据解决方案是面对需求的，比卖产品 feature 更能吸引客户签单。从厂商角度看，<strong>解决方案能打破客户采购的对标价格体系，通过创造差异化来提高议价能力。</strong>在成熟行业做标品生意，竞品之间的价格基本上是透明的，企业客户在采购过程中的议价能力非常强，也容易碰到一些低端厂商挑起的价格战。但解决方案模式，每家提供的服务、价格都是不一样的。</p><p>最后，大数据解决方案这个商业模式要做好，会碰到非常多的问题。比如如何积累团队的业务咨询能力和行业知识，如何处理人力大幅增加带来的薪酬和管理成本问题，如何建立高效且有能力的项目实施交付团队，是否需要合作伙伴一起做落地以及如何选择，如何解决合作伙伴间配合的问题，如何维护客户关系，如何控制整体成本，如何从项目中沉淀可复制的解决方案，而不是做一个算一个沦为外包服务商，如何保证核心产品的通用性，防止大客户的定制需求拖死产品研发团队等等坑都是要趟过的。</p><p>不是每个企业都适合做这个事，要深入分析自己企业的能力、发展方向，不能看到别人这么做自己想都不想也这么做。</p><section class="footnotes" role="doc-endnotes"><hr /><ol><li id="fn1" role="doc-endnote"><p>本文仅代表个人观点。<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
    
    
    <summary type="html">&lt;p&gt;年初（2018）接受 DTalk 社区访谈 ，对“大多数企业怎样把大数据落地变现？”这个问题，我当时是这么回答的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;我理解的大数据落地变现有两大模式，一种是基于大数据技术，另一种是基于已有的数据资产。大体有如下几种方式：&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;输出平台型技术能力，通过给企业建设大数据平台来变现；&lt;/li&gt;
&lt;li&gt;输出大数据处理技术和应用产品，比如把企业内部的BI、应用/用户分析、营销监测、数仓应用等产品进行商业化输出或者通过数据建模咨询和实施来变现。&lt;/li&gt;
&lt;li&gt;基于数据的闭环服务变现，如营销方向的广告精准推送、金融领域的风控服务等。&lt;/li&gt;
&lt;li&gt;咨询类的数据报告，针对不同领域提供对客户有价值的分析及数据报告等。&lt;/li&gt;
&lt;li&gt;数据交易。&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>关于数据采集和用户行为分析平台的一些问题</title>
    <link href="http://blog.zhengdong.me/2018/04/04/questions-on-data-collection-and-data-platform/"/>
    <id>http://blog.zhengdong.me/2018/04/04/questions-on-data-collection-and-data-platform/</id>
    <published>2018-04-04T12:48:27.000Z</published>
    <updated>2020-11-28T10:52:32.833Z</updated>
    
    <content type="html"><![CDATA[<p>最近要参加一个关于数据埋点和分析的线上讨论，这两天总结了对一些问题的思考。</p><h2 id="为什么企业需要一套完善的用户行为埋点和分析平台">为什么企业需要一套完善的用户行为埋点和分析平台？</h2><p>一个互联网产品从萌芽到发展壮大，离不开对用户行为的深度洞察。</p><p>产品初创期间，需要分析天使用户的行为来改进产品，甚至从用户行为中得到新的思路或发现来调整产品方向；产品 growth 过程，通过对用户行为的多角度（多维）分析、对用户群体的划分以及相应行为特征的分析和比较，来指导产品设计、运营活动，并对市场渠道效果进行评估。</p><p>配合上 A/B 试验平台，可以加速产品的迭代，更快得到用户的真实反馈。同时，这些数据沉淀下来，对业务的数据仓库建设、数据智能应用等方面也能起到促进作用，比如做实时推荐，需要能更快获得用户尽可能多且明细的行为数据；做用户分类、意愿预测等机器学习业务，需要清洗过的规范化、结构化的数据做 training。</p><p>要能做用户行为的分析，就需要有一套用户行为数据采集、传输、处理、分析的基础设施，而埋点和分析平台就是在做这件事。业界大多产品都是通过嵌入到多个终端的 SDK 来采集用户行为数据，而后续的传输、处理等过程对需求方是透明的，这样可以以很低的成本，把数据的采集、清洗、沉淀工作做掉，为企业节省成本，提升数据驱动的效率。</p><p>在分析平台上，用户的行为定义会通过特定 <code>Event</code> 来标识，比如 "buttonClick", "playMusic" 等。通常这些事件，是开发人员通过调用 SDK 提供的 API 来设置的，除了确定事件的名称外，还可以加入分析需要的自定义参数和取值，这个过程就是“埋点”工作。当然，还有一些工具/产品支持可视化埋点，这种方式不需要开发介入埋点，SDK 会自动去采集用户在各个终端上的行为。</p><h2 id="代码埋点可视化埋点和无埋点有哪些区别在使用过程中该如何选择">代码埋点、可视化埋点和无埋点有哪些区别，在使用过程中该如何选择？</h2><p>分析平台通常会提供各端的数据采集 SDK，代码埋点是产品开发者通过调用这些 SDK 提供的一些 API 来记录用户行为的方式，及俗称的“埋点”或者“打点”。</p><p>如 <code>trackEvent("buttonClick")</code></p><p>可视化埋点是指开发人员除集成采集 SDK 外，不需要额外去写埋点代码，而是由业务人员通过访问分析平台的 <code>圈选</code> 功能来“圈”出需要对用户行为进行捕捉的控件，并给出事件命名。圈选完毕后，这些配置会同步到各个用户的终端上，由采集 SDK 按照圈选的配置自动进行用户行为数据的采集和发送。</p><p>无埋点是指开发人员集成采集 SDK 后，SDK 便直接开始捕捉和监测用户在应用里的所有行为，并全部发送到分析平台，不需要开发人员添加额外代码。在分析时，业务人员通过分析平台的圈选功能来选出自己关注的用户行为，并给出事件命名。之后便可以对特定用户行为（事件）进行多维分析了。</p><p>可视化埋点和无埋点比较像，都不需要开发人员手工加代码，也都需要业务人员进行所关注的用户行为的圈选。两者最大的不同是在用户终端的表现上，可视化埋点只采集业务人员关注的用户行为数据，而无埋点是会采集所有用户的行为数据，通常情况下数据量后者比前者大很多。</p><p>也正是由于无埋点默认采集所有用户行为数据，它能够做到事件的<code>回溯</code>分析，即在业务人员新定义（圈选）事件后，就能去分析这个事件在前面一两个月的数据情况，这也是可视化、代码埋点支持不了的。但带来的问题就是采集所有数据对应用的侵入会有些大，也会增大用户端采集的数据量。当然，可以通过一些策略，比如 Wi-Fi 下才发缓解这些问题。</p><p>无埋点和可视化埋点很大一个缺陷在于它们都是通过采集 SDK 去监测应用上控件的触发事件（用户对控件的操作），当产品 UI 在版本升级过程发生变动，或者产品做了大的改版，一些行为的“埋点”会发生丢失。如控件ID发生变化，而圈选的配置没变，导致数据采集不到；或者和业务的实际需要发生不一致的变动，比如圈选控件的作用发生了变化，但圈选配置没改；这些问题会导致对产品某些方面的分析出现差错，往往查起来还比较麻烦，在技术上完全解决也比较困难。</p><p>另外，可视化埋点和无埋点都针对的是客户端数据采集，一些用户行为数据在客户端是采集不到的，或者客户端采集的精准度不够，比如支付，因为支付成功的判断绝大多数场景都是在服务端做的，所以在客户端做支付行为的埋点，误差很大，这个时候就需要在服务端进行埋点。</p><p>我的建议是，在产品初期，产品形态还不太稳定、分析的复杂度还比较低的阶段，采用无埋点或者可视化埋点，更快去做埋点，否则频繁的产品改动，会让开发人员大量时间花在琐碎的埋点代码维护上面。产品进入稳定期后，尽量采用代码埋点方式，可以保证事件模型是稳定的，便于长期的数据监控、分析和数据沉淀。</p><h2 id="如何进行数据埋点方案及规范的定义以及后续怎么进行维护和管理">如何进行数据埋点方案及规范的定义，以及后续怎么进行维护和管理？</h2><p>一个互联网产品业务数据驱动的 workflow 往往是这样的：</p><ol type="1"><li>定义产品的阶段性目标；</li><li>规划和定义指标，包括产品、运营、市场的各项目标；</li><li>产品、运营等业务人员确定数据埋点需求；</li><li>开发人员进行埋点以及数据的上报等开发工作；</li><li>数据开发人员进行数据的清洗、宽表建设、指标计算等工作；</li><li>业务人员分析数据、发现产品问题或潜在机会；</li><li>继续下一阶段的产品、运营、市场等的改进工作。</li></ol><p>用户行为分析平台的目标就是将其中 4-6 阶段的工作变得简单和自动化，把开发人员解放出来去做更多对业务有价值的工作。而 1-3 部分的工作，看起来不复杂，基于业务现状去定义指标，排出埋点需求，和开发人员确认好就完成了。</p><p>但这块从实践上来看，很多企业或者业务都做的不够好。比如定义的事件数量迅速膨胀，一段时间后，团队可能大部分人都不知道某些埋点是做什么的，开发人员也不好删掉，就一直存在着，可能早已失去了业务价值；或者业务人员定义了埋点需求，但开发人员埋点做错了，彼此都没发现，导致分析过程出现错误解读；又或者上线了才发现埋点的参数或者位置不对，但又必须得等到下一次发版本才能解决。</p><p>这块有几件事情可以做：</p><ul><li>指标管理系统，用来维护指标依赖的数据表、字段以及计算方式，来统一开发、分析和解读过程的口径。</li><li>埋点管理系统，用来管理埋点的元数据，包括事件 <code>Event</code> 的命名、自定义字段含义和特定取值等规范定义，埋点在产品端的位置或触发场景，埋点工作流等，作为业务人员、开发者、分析师沟通的桥梁和基准。</li><li>埋点测试和校验系统，提供 debug 工具方便开发人员快速进行埋点调试，以及使用事件定义的规范要求，在线上对埋点数据进行校验，尽早发现不符合规范的数据，提高埋点工作的效率和准确性。</li></ul><h2 id="如何做好埋点工作和研发的协调和落地">如何做好埋点工作和研发的协调和落地？</h2><p>在实践中，很多开发人员不太愿意做“埋点”的工作，觉得很琐碎，而且随着产品的发展，包袱有时候会越来越大，维护的工作量不小。</p><p>要让埋点工作在研发比较好的落地，最能提升的地方还是在于如何简化开发人员的工作，包括开发成本和沟通成本。</p><p>采集 SDK 应该尽量简化 API，能自动做的就不要让开发人员来做，比如应用生命周期的检测、PageView 的采集、甚至对一些企业内部组件化框架的支持，尽可能减少开发人员接入分析平台需要添加的代码量。</p><p>有完善的埋点管理系统，这样研发端可以依据进行开发，减少“口口相传”带来的低效和返工，也能统一口径和进度流程。</p><p>有高效易用的埋点测试、校验系统，开发人员可以快速进行埋点 debug，提高开发效率，也能让业务方尽早介入需求校验，而不是等应用真正发布后才去校验，去发现问题。</p><p>当然，最好能和开发人员持续分享数据是如何促进业务的发展，让大家明白这些工作的价值，才能更重视，更认真对待这部份工作。</p><h2 id="埋点数据采集与企业数据资产建设怎样更好的合作">埋点数据采集与企业数据资产建设怎样更好的合作？</h2><p>用户行为分析平台在建设时，数据端会包含如下能力：</p><ul><li>数据接入，要支持客户端、Web、服务端等多终端的数据采集，如 iOS、Android、微信小程序等，以及各种数据源甚至三方服务的数据适配。</li><li>数据传输，在用户规模和数据规模增长过程中，要能保证数据传输服务的高可用、以及采集数据在传输过程的及时性。</li><li>数据建模/存储，要能实时的进行数据清洗、建模和存储落地。</li></ul><p>这些能力，在互联网业务的数据资产建设过程中，尤其是用户、流量、产品相关领域，能起到基础设施的作用。规范的数据采集，加上高效的传输、建模能力，是企业业务数据资产有效建设的前提。</p><p>建模后的数据，可以作为数据仓库底层（ODS 层）的宽表，和企业的其他业务数据整合，共同完善企业的数据资产建设。</p><p>另一方面，这些用户端的结构化数据，加上实时建模和开放的能力，和机器学习算法结合起来，无论是个性化推荐，还是精准营销，又或是银行、电商的风控，都可以发挥很大威力，为企业的智能驱动业务做好数据积累，扫清障碍。</p><p>企业在数据建设的过程中的产出，也可以扩充以 PaaS 提供服务的用户行为分析平台的能力，让企业在平台上可以做更多的事情，如 CRM、推送、实时推荐等。</p><p>拿 DMP （用户画像）建设举个例子，</p><p>企业在建设自己的 DMP 库的过程中，常常会从常规的人口属性等准静态类标签，以及像消费能力等从自身业务积累或三方合作得到的通用类标签入手。这些标签往往是泛业务的，针对具体业务而言，很多时候会需要用户画像标签更贴近业务，比如电商业务场景下的母婴用户、电子产品发烧友、化妆品品牌喜好用户等。这些标签和用户的发掘，需要对用户的行为进行深度分析来获取，这个工作便可以借助用户行为分析平台的能力，如基于用户行为模式和用户业务属性对用户进行分群分析和比较，来发现和挖掘有价值的用户标签。</p><p>另一方面，用户画像的数据，也可以和分析平台进行整合和集成，提升平台各分析模型对不同用户群的洞见能力，让分析和指标的比较更有针对性，提升数据对业务的促进能力。</p><h2 id="埋点及分析平台和-ab-试验平台如何更好的互相促进">埋点及分析平台和 A/B 试验平台如何更好的互相促进？</h2><p>A/B 测试产品是通过提供专业高效的试验平台，帮助产品进行产品决策的验证和分析。常规使用流程如下：</p><p>接入 SDK -&gt; 创建试验版本 -&gt; 设置变量、以及优化指标 -&gt; 调节试验流量 -&gt; 运行试验 -&gt; 实时监控数据进行效果评估 -&gt; 正式发布</p><p>试验平台和分析平台的 SDK 在很多功能上是重合的，在 SDK 实现上可以整合，减少业务应用接入太多 SDK 的负担。</p><p>在数据采集、建模、分析层面，分析平台可以做为 A/B 试验平台后端数据的承载，优化指标的效果评估就能覆盖用户的全量行为，无需业务及开发人员维护多个工具带来的重复埋点定义和开发工作。另外，在分析平台积累的很多分析模型和指标，在 A/B 试验平台直接可以选取使用，无需在试验平台再进行设置，除减少业务人员工作外，还能保证统计口径的一致。</p><p>反过来，A/B 试验平台的一些对比试验，以及特定灰度发布的用户群，也能整合到分析平台，通过分群分析能力，将这些群体应用到各个分析模型进行针对性的分析，甚至试验结束后，也能持续对这些用户进行追踪和分析，更好的洞察用户。</p><h2 id="如何打通产品多端的埋点数据">如何打通产品多端的埋点数据？</h2><p>目前大多数用户行为分析产品都会通过 SDK 支持 iOS、Android、Web 三个端的数据采集，还有些产品覆盖的更广，支持 PC、微信小程序、服务端、甚至直接基于 HTTP API 进行数据采集。</p><p>在分析如何打通多端用户数据前，我想先谈下单个终端的用户标识问题，毕竟，如果单个端的用户标识都不稳定，那多端用户数据打通也就失去了意义。</p><p>现在的分析产品在一般情况下，移动端会通过 SDK 生成唯一 ID 来标识用户/设备。移动化发展早期，很多采集工具用过 mac address、IDFA、android_id、IMEI 等从移动操作系统可以获取的设备软硬件信息来标识设备，但随着操作系统的发展，很多信息获取接口要么被封禁，要么已经失去了精准性。反倒是一开始就通过自己生成的 ID 来标识用户的工具，受到的影响不大，基本保持了用户/设备标识的稳定。</p><p>但这种方式有个问题，在用户卸载、重装或者刷机后，ID 信息会丢失，导致生成新的用户/设备 ID。这个问题，可以通过 ID Mapping 技术来解决：</p><p>分析平台对每个用户生成一个虚拟 ID，对同一个用户的多个设备和帐号进行映射，并绑定起来。</p><ul><li>可以通过操作系统提供的一些稳定性稍差，但短时间还比较稳定的指标，如 iOS 的 IDFA，来做 mapping。</li><li>借助分析产品的应用覆盖率，如用户是应用 A 和 B 的用户，卸载并重新安装 B 应用后，可以通过应用 A 的 ID 修复应用 B 的。</li><li>通过引入产品用户帐号体系，来做绑定，这种方式稳定性最强，但非登录匿名用户的问题不好解决。</li><li>通过 IP、Wi-Fi 信息、机器型号、甚至地理位置进行 mapping，这种方式需要用户授权更多数据获取权限，虽然是近似匹配，但当信息足够多且发散（信息熵足够大）时，也可以起到统一标识的作用。</li></ul><p>通过这个虚拟 ID 实质上就打通了产品的多端数据。实践中，ID Mapping 体系的建设工作量不小，Mapping 后用户标识如果需要发生调整，在基于事件的分析产品上需要对老数据进行重写，比较复杂。所以对于一些强帐号体系的产品，可以退化到只用用户帐号来做关联，只有非登录匿名用户才用设备 ID 来标识，这往往是性价比比较高的方案。</p><p>再引申下，在多端数据打通问题的讨论中，经常会提到用户来源归因的问题。</p><p>比如，产品做推广，使用了百度 SEM、广点通、应用市场等渠道，想知道各个渠道有多少用户激活了，以及后续使用情况如何。</p><p>为了解决这个问题，支持营销效果评估的分析平台会要求产品在平台上生成推广链接进行投放。用户在点击链接时，会从分析平台的域下做跳转再到目标页，这样就可以借助浏览器的 cookie 机制进行匹配，来对用户来源进行归因，但这种方式在移动端上面的表现不太好（iOS 已经取消了 SFSafariViewController 多应用共享 cookie 的支持）。除此之外，也可以采用 ID Mapping 提到的近似匹配技术，很多厂商声称的设备指纹技术大多也是这种，不太准，但定性分析是可以的。</p><p>一些做移动业务比较多的推广渠道，支持设备 ID 的回传功能来方便产品归因问题的解决。产品方在投放链接的时候，遵照特定格式即可。</p><p>如 <code>https://xxx.com/aaaafD?idfa=__IDFA__&amp;imei=__IMEI__</code></p><p>渠道在用户点击广告链接后，会把设备 ID 如 IDFA 或 IMEI 加到链接的内容里面，用户激活后便可以通过相应 ID 匹配来归因。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近要参加一个关于数据埋点和分析的线上讨论，这两天总结了对一些问题的思考。&lt;/p&gt;
&lt;h2 id=&quot;为什么企业需要一套完善的用户行为埋点和分析平台&quot;&gt;为什么企业需要一套完善的用户行为埋点和分析平台？&lt;/h2&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Readings in Database Systems - Interactive Analytics</title>
    <link href="http://blog.zhengdong.me/2016/08/08/readings-in-database-systems-interactive-analytics/"/>
    <id>http://blog.zhengdong.me/2016/08/08/readings-in-database-systems-interactive-analytics/</id>
    <published>2016-08-08T04:04:11.000Z</published>
    <updated>2020-11-28T10:52:32.832Z</updated>
    
    <content type="html"><![CDATA[<p>最近在看 <a href="https://en.wikipedia.org/wiki/Michael_Stonebraker">Stonebraker</a> 的 <a href="http://www.redbook.io">“Readings in Database Systems”</a>, 发觉开拓了很多思路。</p><p>这么多年自己一直在从事大数据方面的工作，但除了翻过数据挖掘算法和分布式系统设计方面的论文外，完全没想过去翻翻数据库相关的论文看。现在想想，其实大数据和数据库两者很多需求和场景是一致的，要解决的问题，没准学术界很多年前就已经有方案了。</p><p>这篇文章主要是 "Interactive Analytics" 相关部分。</p><h2 id="what-is-interactive-analytics">What is Interactive Analytics</h2><p>假如你是一家电商公司的分析师，如果有 100 万用户原始交易数据打印出来摆在你面前，让你去分析这些数据的意义，你会怎么做？</p><p>如果这十万条数据给我，我估计是看不出什么东西出来。而且我相信每个人，也是如此，因为人的认知是有 bug 的，比如不能直接处理大量原始数据。</p><p>那该怎么办？</p><p>我们需要把数据通过一些方式做提炼，变成小的结果集，或者以可视化的形式展现出来。用过 <code>SQL</code> 的人也许会想到 <code>Group by</code> 语句，是的，往往通过 <code>Group by</code> 做 aggregation 后的数据，会好理解很多。</p><p><strong>大数据不只是数据量超大，更在于能从大量数据里面发现价值。</strong></p><p>而 "Interactive Analytics" 指的就是这个过程，但加了个前提：这个过程必须能在较短的时间内完成，哪怕甚至来不及遍历所有需要的原始数据。</p><p>当数据量超大的时候，这个前提对每个数据系统都是一个很大的挑战。</p><h2 id="ideas">Ideas</h2><p>那怎么让一个查询请求的执行过程比直接遍历所有依赖的数据还快呢？</p><p>结论，显而易见，只能不去遍历所有的依赖数据，能有这样的方案，那问题也就迎刃而解了。</p><p>目前靠谱的方案有两种：</p><ol type="1"><li><code>Precomputing</code>，如果预先把查询请求依赖的相关数据都做了一定的 “提炼”，便可大大减少查询需要去遍历的数据。</li><li><code>Sampling</code>，可以对数据进行取样，每次查询请求都只遍历取样后的数据，这样遍历数据也可以大大减少。</li></ol><p>“Red Book” 给出了四篇参考文献，<a href="#ref1">1</a>，<a href="#ref2">2</a>是关于 <code>Precomputing</code> 的，<a href="#ref3">3</a>，<a href="#ref4">4</a>是关于 <code>Sampling</code> 的。</p><h2 id="precomputing">Precomputing</h2><h3 id="data-cube">Data Cube</h3><p>还是以之前的交易数据举例，假如我们只关注零部件（Part）、供应商（Supplier）和客户（Customer）三个维度，关注的指标是总销售额，那么我们预先可以分别把每个不同部件 p、供应商 s、客户 c 的销售额总和统计出来，以 <code>(p, s, c)</code> 形式存起来，如果有相关查询请求，直接返回结果就可以了。</p><p>这就是一个三维 Data Cube 的建立和使用，每一个 cell 代表一种部件、供应商、客户组合 <code>(p, s, c)</code>，对应的 value 就是这个组合的销售额总计。</p><h3 id="build-data-cube">Build Data Cube</h3><p>当然，实际情况下，分析任务关注的维度肯定不仅是三个，可能是多个不同的维度组合。</p><p>对 data cube 的建立有如下三种方式：</p><ol type="1"><li>预先计算出所有组合的 data cube，之后所有的请求就可以得到最快的响应，但会带来很大的预计算和数据存储压力。（如果有 K 的维度，需要执行 <span class="math display">\[2^k\]</span> 个 Group by 语句来做预计算。）</li><li>不做任何预计算，每个请求都直接从原始数据进行提取，这种方式没有额外的数据存储压力，但数据量大的情况下请求执行耗时会非常长。</li><li>预先计算一些维度组合的 data cube，这个是 <a href="#ref1">1</a> 采取的方式，这种方式目标是做到请求执行耗时、预计算耗时和存储的平衡，但选择哪些维度组合做预计算是关键，选择错了，可能还不如采用上面两种方式。</li></ol><p>前面的例子，要全部预计算出部件、供应商和客户三个维度的 data cube，需要如下 8 个组合：</p><ol type="1"><li>psc (part, supplier, customer) (6M: 6 million rows)</li><li>pc (part, customer) (6M)</li><li>ps (part, supplier) (0.8M)</li><li>sc (supplier, customer) (6M)</li><li>p (part) (0.2M)</li><li>s (supplier) (0.01M)</li><li>c (customer) (0.1M)</li><li>none (1)</li></ol><p>（组合后面的数字代表该组合所有结果数据的行数）</p><p>可以发现，其实如果 <code>psc</code> 的数据有了，<code>pc</code> 可以通过按 <code>supplier</code> 维度汇聚 <code>psc</code> 的数据得到，<code>p</code> 可以通过按 <code>cutomer</code> 维度汇聚 <code>pc</code> 的数据得到，其他依次类推。</p><p><code>pc</code> 组合和 <code>psc</code> 组合都有 6 百万行记录，也就是对 <code>pc</code> 和 <code>psc</code> 两个维度组合进行查询都要遍历这么多行记录，那么如果不预计算 <code>pc</code>，而在用户请求 <code>pc</code> 维度时直接通过对 <code>psc</code> 维度进行汇聚，遍历的数据行数是一致的，如果以数据行数作为衡量指标，预计算 <code>pc</code> 便是毫无必要的。</p><h3 id="the-lattice-framework">The Lattice Framework</h3><p><a href="#ref1">1</a> 中提出了一个 Lattice 模型，如下图所示：</p><p><a><img src="/images/2016/the-lattice-framework.png" width="560"></a></p><p>每个节点表示一个 data cube 组合，下方的节点可以通过上方节点汇聚得到。</p><p>左边是上面例子的 lattice 模型，右边是模型之间的合并过程。</p><p>通过这样的结构，可以将维度组合选择转化为一个最优选择的问题：</p><p><em>在限制节点个数的情况下，最小化每个节点预计算的平均耗时。</em></p><p><a href="#ref1">1</a> 中首先提出了个 cost model 来评估通过依赖节点计算自身 data cube 的 cost，然后提出了个 greedy algorithm 通过计算平均最少 cost 来进行预计算维度选择，算法的细节和证明大家可以细读该<a href="http://web.eecs.umich.edu/~jag/eecs584/papers/implementing_data_cube.pdf">论文</a>。</p><p><a href="#ref2">2</a> 中给出了一种基于内存的 data cube 计算方法，有兴趣可以<a href="http://pages.cs.wisc.edu/~nil/764/DADS/38_zhao97arraybased.pdf">下载阅读</a>。</p><h2 id="sampling">Sampling</h2><p>Data Cube 模式，不论如何优化，都是需要离线任务去预先构建大量的 Cube 集，在需要的维度很多、或者数据延迟要求很低的场景下，不能很好的满足要求。</p><p>Sampling 方式是在降低准确性的前提下，减少遍历的数据量，达到快速响应查询请求的目的。</p><p><a href="#ref4">4</a> 中通过对用户的查询请求进行统计，评估出经常用的查询列集合，预先进行 Sample 创建。</p><h3 id="sample-creation">Sample Creation</h3><p>如何进行 Sample 创建，我在看论文的时候，直接想到的是将数据记录打乱，随机分布在若干个 partition 里面，当有请求过来的时候，直接选择一个或多个 partition 进行查询即可。</p><p><a href="#ref4">4</a> 中提到了这样做（<a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)#Sampling_from_a_uniform_distribution">uniform sampling</a>）的问题：</p><p><em>如果只是全局的对数据做统计，效果比较好，但如果有 filter 或者 group by 操作，这种方式往往得不到好的效果。</em></p><p>举个例子，比如我要按城市来统计销售额的分布，如果是采用我想的那种分布方式的话，一些交易量很少的城市，可能在 sample 里完全消失了，这样的分布统计，其实是错的。</p><p><a href="#ref4">4</a> 中提出了 <a href="https://en.wikipedia.org/wiki/Stratified_sampling">Stratified Sampling</a> 方式来解决这个问题。</p><p>基本思想就是首先对维度列进行统计，将相同列值的行作为一个 group，然后分别进行 sampling，论文中详细介绍了 sampling 的方法和每个 group sampling size 的设定。</p><h3 id="sample-selection">Sample Selection</h3><p>在如何选择 Sample 的问题上，<a href="#ref4">4</a> 提出了 ELP（Error Latency Profile）模型，通过用户设定的准确率和耗时要求，进行 sample 选择。</p><p>当然，这是个非常复杂的过程。<a href="#ref4">4</a> 中详细讲了如何去评估各个 Sample 的耗时和准确率，怎么样在生成执行计划的过程中考虑用户的准确率和耗时的要求。有兴趣大家可以<a href="https://sameeragarwal.github.io/blinkdb_eurosys13.pdf">详细阅读</a>。</p><p><a href="#ref3">3</a> 通过提出的如随机数据访问、在线排序、ripple join 等算法，在已有的关系型数据库，实现了一套支持 online sampling 的原型系统，有兴趣可以<a href="https://www.semanticscholar.org/paper/Informix-under-CONTROL-Online-Query-Processing-Hellerstein-Avnur/367845b3e2f4dd6e36d7725a579575f8855d9737/pdf">详细阅读</a>。</p><h2 id="summary">Summary</h2><p>Data Cube 方案，在数据的准确度方面是毋须质疑优于 Sampling 方案的，工程界的 <a href="http://kylin.apache.org">Apache Kylin</a> 就是如此的方式。而 Sampling 方式目前的应用还比较少，对于很多用户而言，Sampling 的方案即使是 99% 的准确度，还是无法接受的，哪怕其实已经满足了他的需求。</p><p>但我倒比较看好 Sampling 方式，因为 Data Cube 的整个机制对数据变化和实时方面有很大的限制，随着内存越来越廉价，以及越来越好的列存储方案，数据进行实时交互分析变得越来越可行，比如 <a href="http://impala.io">Impala</a> 和 <a href="https://prestodb.io">Presto</a>，在大数据量的情况下，性能都很好，不大的集群都可以做到秒级响应对亿级数据量的查询。</p><p>当然，资源不可能是无限的，也不可能每个查询请求都能有资源保证快速遍历海量数据，所以，通过对准确率方面的牺牲，达到查询耗时的降低，其实是一种比较经济的方案 (Presto 是有类似 <a href="#ref4">4</a> 中提到的 Sampling 方案）。</p><p>Reference</p><p><a name="ref1">[1] Venky Harinarayan, Anand Rajaraman, Jeffrey D. Ullman. Implementing Data Cubes Efficiently. SIGMOD, 1996.</a></p><p><a name="ref2">[2] Yihong Zhao, Prasad M. Deshpande, Jeffrey F. Naughton. An Array-Based Algorithm for Simultaneous Multidimensional Aggregates. SIGMOD, 1997.</a></p><p><a name="ref3">[3] Joseph M. Hellerstein, Ron Avnur, Vijayshankar Raman. Informix under CONTROL: Online Query Processing. Data Mining and Knowledge Discovery, 4(4), 2000, 281-314.</a></p><p><a name="ref4">[4] Sameer Agarwal, Barzan Mozafari, Aurojit Panda, Henry Milner, Samuel Madden, Ion Stoica. BlinkDB: Queries with Bounded Errors and Bounded Response Times on Very Large Data. EuroSys, 2013.</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近在看 &lt;a href=&quot;https://en.wikipedia.org/wiki/Michael_Stonebraker&quot;&gt;Stonebraker&lt;/a&gt; 的 &lt;a href=&quot;http://www.redbook.io&quot;&gt;“Readings in Database Systems”&lt;/a&gt;, 发觉开拓了很多思路。&lt;/p&gt;
&lt;p&gt;这么多年自己一直在从事大数据方面的工作，但除了翻过数据挖掘算法和分布式系统设计方面的论文外，完全没想过去翻翻数据库相关的论文看。现在想想，其实大数据和数据库两者很多需求和场景是一致的，要解决的问题，没准学术界很多年前就已经有方案了。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>A Bug in a Java Servlet</title>
    <link href="http://blog.zhengdong.me/2014/06/20/a-bug-in-a-java-servlet/"/>
    <id>http://blog.zhengdong.me/2014/06/20/a-bug-in-a-java-servlet/</id>
    <published>2014-06-20T03:47:33.000Z</published>
    <updated>2020-11-28T10:52:32.830Z</updated>
    
    <content type="html"><![CDATA[<p>We have a legacy system, which is a web service, receives HTTP POST from clients, parses the data, then stores them in a file.</p><p>The function of the system is simple, and people already done functional and performance test, it's stable. As time drifted away, the system was <a href="http://en.wikipedia.org/wiki/Copy_and_paste_programming">copy and paste</a> to some projects by only changing the data parsing logic.</p><p>I had a similar requirement recently, then I delved into the legacy code to check if it works in order to not <a href="http://en.wikipedia.org/wiki/Reinventing_the_wheel">reinventing the wheel</a>.</p><h2 id="wtf">WTF</h2><p>At first, I noticed below code in a <code>HttpServlet</code> class, it allocates more than 1M memory for each HTTP POST request.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">   <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> MAX_CONTENT_LENGTH = <span class="number">1024</span> * <span class="number">1024</span>;</span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> BUFFER_SIZE = <span class="number">4096</span>;</span><br><span class="line"></span><br><span class="line">   ...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">doPost</span><span class="params">(HttpServletRequest request, HttpServletResponse response)</span></span></span><br><span class="line"><span class="function"><span class="keyword">throws</span> ServletException, IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">       ...</span><br><span class="line"></span><br><span class="line">       <span class="keyword">int</span> requestContentBufSize = request.getContentLength() + MAX_CONTENT_LENGTH;</span><br><span class="line">       ByteBuffer requestContentBuf = ByteBuffer.allocate(requestContentBufSize);</span><br><span class="line">       <span class="keyword">byte</span>[] buffer = <span class="keyword">new</span> <span class="keyword">byte</span>[BUFFER_SIZE];</span><br><span class="line">       requestInputStream = <span class="keyword">new</span> DataInputStream(request.getInputStream());</span><br><span class="line">       <span class="keyword">int</span> readBytes = <span class="number">0</span>;</span><br><span class="line">       <span class="keyword">int</span> totalReadBytes = <span class="number">0</span>;</span><br><span class="line">       <span class="keyword">while</span> ((readBytes = requestInputStream.read(buffer)) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">           requestContentBuf.put(buffer);</span><br><span class="line">    totalReadBytes = totalReadBytes + readBytes;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">byte</span>[] requestContent = Arrays.copyOf(requestContentBuf.array(), totalReadBytes);</span><br><span class="line"></span><br><span class="line">       ...</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>It's insane, I believe the memory should be the same as each HTTP POST body size. Then I changed the code.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> requestContentBufSize = request.getContentLength();</span><br></pre></td></tr></table></figure><p>Deployed the service and sent one HTTP POST request to it.</p><p><code>curl -d 'Hello, World' http://my.server.com:9000/log</code></p><p>An <code>Exception</code> occurred.</p><h2 id="the-bufferoverflowexception">The BufferOverflowException</h2><p>After reducing the memory allocated for <code>ByteBuffer</code>, it overflows.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">java.nio.BufferOverflowException</span><br><span class="line">at java.nio.HeapByteBuffer.put(HeapByteBuffer.java:183)</span><br><span class="line">at java.nio.ByteBuffer.put(ByteBuffer.java:830)</span><br><span class="line">at com.myproject.servlet.LogServer.doPost(LogServer.java:99)</span><br><span class="line">at javax.servlet.http.HttpServlet.service(HttpServlet.java:643)</span><br><span class="line">at javax.servlet.http.HttpServlet.service(HttpServlet.java:723)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)</span><br><span class="line">at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)</span><br><span class="line">at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)</span><br><span class="line">at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)</span><br><span class="line">at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)</span><br><span class="line">at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)</span><br><span class="line">at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)</span><br><span class="line">at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:861)</span><br><span class="line">at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:606)</span><br><span class="line">at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)</span><br><span class="line">at java.lang.Thread.run(Thread.java:701)</span><br></pre></td></tr></table></figure><p>I thought I'd better dig into how does the servlet do to make <code>ByteBuffer</code> get its data?</p><ol type="1"><li>It creates a small buffer occupied <code>BUFFER_SIZE</code> (4096) bytes.</li><li>It iterates the HTTP request input stream, to put the data into the small buffer.</li><li>It puts the small buffer to <code>ByteBuffer</code> and loop back to <code>1</code>.</li></ol><p>Well, in the last loop, the data read from the HTTP request input stream might smaller than the <code>BUFFER_SIZE</code>, but the servlet still puts <code>BUFFER_SIZE</code> bytes to <code>ByteBuffer</code>.</p><p>Then, to fix the <code>ExceptionBufferOverflowException</code>, I increased the capacity of previous <code>ByteBuffer</code> by <code>BUFFER_SIZE</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> requestContentBufSize = request.getContentLength() + BUFFER_SIZE;</span><br></pre></td></tr></table></figure><p>Deployed again, and</p><p><code>curl -d 'Hello, World' http://my.server.com:9000/log</code></p><p>The bug was fixed.</p><p>Did I?</p><h2 id="the-servletinputstream">The ServletInputStream</h2><p>When client posts huge data, what could happen?</p><p>I created a String which is 7516 bytes, and sent to server.</p><p><code>curl -d 'very very long string' http://my.server.com:9000/log</code></p><p>Sometimes, the <code>java.nio.BufferOverflowException</code> occurred, and sometimes it didn't.</p><p>What went wrong?</p><p>To find the root cause, I added some logs to trace the <code>ByteBuffer</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">int</span> requestContentBufSize = request.getContentLength() + BUFFER_SIZE;</span><br><span class="line">  ByteBuffer requestContentBuf = ByteBuffer.allocate(requestContentBufSize);</span><br><span class="line">  <span class="keyword">byte</span>[] buffer = <span class="keyword">new</span> <span class="keyword">byte</span>[BUFFER_SIZE];</span><br><span class="line">  requestInputStream = <span class="keyword">new</span> DataInputStream(request.getInputStream());</span><br><span class="line">  <span class="keyword">int</span> readBytes = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> totalReadBytes = <span class="number">0</span>;</span><br><span class="line">  log.debug(<span class="string">&quot;1: ByteBuffer position: &quot;</span> + requestContentBuf.position() +</span><br><span class="line">          <span class="string">&quot;, buffer capacity: &quot;</span> + requestContentBuf.capacity() +</span><br><span class="line">          <span class="string">&quot;, buffer remaining: &quot;</span> + requestContentBuf.remaining());</span><br><span class="line">  <span class="keyword">while</span> ((readBytes = requestInputStream.read(buffer)) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">requestContentBuf.put(buffer);</span><br><span class="line">totalReadBytes = totalReadBytes + readBytes;</span><br><span class="line">      log.debug(<span class="string">&quot;2. Bytes read: &quot;</span> + readBytes);</span><br><span class="line">      log.debug(<span class="string">&quot;1: ByteBuffer position: &quot;</span> + requestContentBuf.position() +</span><br><span class="line">              <span class="string">&quot;, buffer capacity: &quot;</span> + requestContentBuf.capacity() +</span><br><span class="line">              <span class="string">&quot;, buffer remaining: &quot;</span> + requestContentBuf.remaining());</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>The log printed when no exception,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- 1: ByteBuffer position: 0, buffer capacity: 11612, buffer remaining: 11612</span><br><span class="line">- 2. Bytes read: 4096</span><br><span class="line">- 1: ByteBuffer position: 4096, buffer capacity: 11612, buffer remaining: 7516</span><br><span class="line">- 2. Bytes read: 3420</span><br><span class="line">- 1: ByteBuffer position: 8192, buffer capacity: 11612, buffer remaining: 3420</span><br></pre></td></tr></table></figure><p>The log printed when exception occurred,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- 1: ByteBuffer position: 0, buffer capacity: 11612, buffer remaining: 11612</span><br><span class="line">- 2. Bytes read: 1356</span><br><span class="line">- 1: ByteBuffer position: 4096, buffer capacity: 11612, buffer remaining: 7516</span><br><span class="line">- 2. Bytes read: 1356</span><br><span class="line">- 1: ByteBuffer position: 8192, buffer capacity: 11612, buffer remaining: 3420</span><br></pre></td></tr></table></figure><p>Now, it is easy to find out the root cause is in these lines of code.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> ((readBytes = requestInputStream.read(buffer)) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    requestContentBuf.put(buffer);</span><br></pre></td></tr></table></figure><p>The <code>read</code> method call won't put data to the <code>buffer</code> fully which was specified as 4096 bytes even when the input stream still has data.</p><p>And to fix it, just specify the offset and length of the small <code>buffer</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> ((readBytes = requestInputStream.read(buffer)) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    requestContentBuf.put(buffer, <span class="number">0</span>, readBytes);</span><br></pre></td></tr></table></figure><p>I had increased the capacity of the <code>ByteBuffer</code> by <code>BUFFER_SIZE</code>, this change should also be reverted.</p><p>Now, the bug is fixed, and this is network programming.</p><h2 id="questions">Questions</h2><p>"The system works a long time, and it shouldn't have this problem or we knew it long ago"</p><blockquote><p>This is because the client seldom posts data more than 4096 bytes to server.</p></blockquote><p>"I have read the Javadoc of <code>DataInputStream</code>, the <code>read</code> method will put data fully to the specified buffer"</p><blockquote><p>It didn't, please read it again.</p></blockquote><p>"I have tested the <code>read</code> method of <code>DataInputStream</code> on a file, it reads fully 4096 bytes in every iteration"</p><blockquote><p>This is a web service, deploy it to a server and test.</p></blockquote><p>"I have tested it on my local machine as a web service, and it reads fully 4096 bytes in every iteration"</p><blockquote><p>This is a web service, it should be in a network.</p></blockquote><h2 id="at-last">At Last</h2><p>When a potential bug was reported, we do tests to make it happen again and find the root cause.</p><p>We do not stop listening and just look for reasons to reject it.</p><p>When we find a bug, we do help others to make it reappear to collect information.</p><p>We do not sit there and just blame on others for their mistakes.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;We have a legacy system, which is a web service, receives HTTP POST from clients, parses the data, then stores them in a file.&lt;/p&gt;
&lt;p&gt;The function of the system is simple, and people already done functional and performance test, it&#39;s stable. As time drifted away, the system was &lt;a href=&quot;http://en.wikipedia.org/wiki/Copy_and_paste_programming&quot;&gt;copy and paste&lt;/a&gt; to some projects by only changing the data parsing logic.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>TDD on Swift</title>
    <link href="http://blog.zhengdong.me/2014/06/16/tdd-on-swift/"/>
    <id>http://blog.zhengdong.me/2014/06/16/tdd-on-swift/</id>
    <published>2014-06-15T16:26:26.000Z</published>
    <updated>2020-11-28T10:52:32.829Z</updated>
    
    <content type="html"><![CDATA[<p>Long long ago, I wrote a post about <a href="/2011/01/16/tdd-using-objective-c">how to do TDD using Objective-C</a>, since <a href="http://www.apple.com/apple-events/june-2014/">Apple WWDC 2014</a>, <a href="https://developer.apple.com/swift/">Swift</a> is really eye-catching, I think I should write a new one to follow the trend.</p><p><a href="https://developer.apple.com/library/ios/documentation/ToolsLanguages/Conceptual/Xcode_Overview/UnitTestYourApp/UnitTestYourApp.html">XCTest</a> is used as the unit test framework, and Xcode 6 is needed.</p><h2 id="tdd-work-flow">TDD Work-flow</h2><ol type="1"><li>Add a test for a user case or a user story</li><li>Run all tests and see if the new one fails</li><li>Write some code that causes the test to pass</li><li>Run tests, change production code until all test cases pass</li><li>Refactor the production code</li><li>Refactor the test code</li><li>Return to 1, and repeat</li></ol><p>The <code>5</code> and <code>6</code> are optional, do them only if needed, but be sure that DO NOT do them at the same time. That is, when you refactor production code, you can't change the test code, until all the test cases are passed, then you are confident that your production code refactoring is perfect, then, you can refactor the test code, and this time, you can't change the production code.</p><h2 id="a-simple-example">A Simple Example</h2><p>We are about to implement a super simple bank account management tool.</p><h3 id="create-a-project">Create a Project</h3><p>Use Xcode to create a project <code>BankAccount</code> (iOS Single View Application)</p><p><img src="/images/2014/tddswift1.png" width="500"></p><h3 id="add-a-test-case">Add a Test Case</h3><p>Create a Swift file named <code>SavingAccountTest</code>, and choose <code>BankAccountTests</code> as target.</p><p><img src="/images/2014/tddswift2.png" width="500"></p><p>"People can deposit money to a saving account", it's our first user story.</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Foundation</span><br><span class="line"><span class="keyword">import</span> XCTest</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SavingAccountTest</span>: <span class="title">XCTestCase</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">func</span> <span class="title">testDeposit</span><span class="params">()</span></span> &#123;</span><br><span class="line">        <span class="keyword">var</span> account = <span class="type">SavingAccount</span>()</span><br><span class="line">        account.deposit(<span class="number">100</span>)</span><br><span class="line">        <span class="type">XCTAssertEqual</span>(<span class="number">100</span>, account.balance)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="run-all-tests">Run All Tests</h3><p>Run all the unit tests, it fails as we expected.</p><p><img src="/images/2014/tddswift3.png" width="500"></p><h3 id="write-code-to-pass-the-test">Write Code to Pass the Test</h3><p>Create a Swift file named <code>SavingAccount</code>, and choose both <code>BankAccount</code> and <code>BankAccountTests</code> as targets.</p><p>Make it simple, just to pass the test.</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Foundation</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SavingAccount</span> </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> balance:<span class="type">Int</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">func</span> <span class="title">deposit</span><span class="params">(money:Int)</span></span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="run-all-tests-1">Run All Tests</h3><p>It passes.</p><p><img src="/images/2014/tddswift4.png" width="500"></p><h3 id="next-user-story">Next User Story?</h3><p>"People could withdraw some money"</p><p>Let's change the <code>testDeposit</code> test case.</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Foundation</span><br><span class="line"><span class="keyword">import</span> XCTest</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SavingAccountTest</span>: <span class="title">XCTestCase</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">func</span> <span class="title">testDepositAndWithdraw</span><span class="params">()</span></span> &#123;</span><br><span class="line">        <span class="keyword">var</span> account = <span class="type">SavingAccount</span>()</span><br><span class="line">        account.deposit(<span class="number">100</span>)</span><br><span class="line">        account.withdraw(<span class="number">50</span>)</span><br><span class="line">        <span class="type">XCTAssertEqual</span>(<span class="number">50</span>, account.balance)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Also, add an empty <code>withdraw</code> method to <code>SavingAccount</code> to satisfy the compiler. Do not add any other code until we see it fails.</p><h3 id="run-all-tests-2">Run All Tests</h3><p>The test fails, because the account balance was not updated after people withdrew some money.</p><p><img src="/images/2014/tddswift5.png" width="500"></p><h3 id="write-code-to-support-withdraw">Write Code to Support Withdraw</h3><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Foundation</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SavingAccount</span> </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> balance:<span class="type">Int</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">func</span> <span class="title">deposit</span><span class="params">(money:Int)</span></span> &#123;</span><br><span class="line">        balance += money</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">func</span> <span class="title">withdraw</span><span class="params">(money:Int)</span></span> &#123;</span><br><span class="line">        balance -= money</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="run-all-tests-3">Run All Tests</h3><p>All the user stories are satisfied.</p><p><img src="/images/2014/tddswift6.png" width="500"></p><h3 id="any-other-new-user-story">Any Other New User Story?</h3><p>"People can't withdraw money beyond their account balance"</p><p>We add a new test case <code>testNegativeBalanceIsNotFine</code></p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">testNegativeBalanceIsNotFine</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> account = <span class="type">SavingAccount</span>()</span><br><span class="line">    account.deposit(<span class="number">50</span>)</span><br><span class="line">    account.withdraw(<span class="number">100</span>)</span><br><span class="line">    <span class="type">XCTAssertEqual</span>(<span class="number">0</span>, account.balance)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="run-all-tests-4">Run All Tests</h3><p>It fails, we have to fix it.</p><p><img src="/images/2014/tddswift7.png" width="500"></p><h3 id="write-code">Write Code</h3><p>Change the <code>withdraw</code> method, set account balance to 0 if it is less than 0.</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">withdraw</span><span class="params">(money:Int)</span></span> &#123;</span><br><span class="line">    balance -= money</span><br><span class="line">    <span class="keyword">if</span> balance &lt; <span class="number">0</span> &#123;</span><br><span class="line">        balance = <span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="run-all-tests-5">Run All Tests</h3><p>All right, all the test cases are succeeded.</p><p><img src="/images/2014/tddswift8.png" width="500"></p><h3 id="refactoring">Refactoring</h3><p>Until now, we haven't do any refactoring on our code base.</p><p>I think the production code is fine, so we skip the step 5, and refactor the test code.</p><p>We can see that both test cases create an instance of <code>SavingAccount</code>, the duplicated code can be removed by using only one <code>SavingAccount</code> instance.</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SavingAccountTest</span>: <span class="title">XCTestCase</span> </span>&#123;</span><br><span class="line">    <span class="keyword">var</span> account = <span class="type">SavingAccount</span>()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">func</span> <span class="title">testDepositAndWithdraw</span><span class="params">()</span></span> &#123;</span><br><span class="line">        account.deposit(<span class="number">100</span>)</span><br><span class="line">        account.withdraw(<span class="number">50</span>)</span><br><span class="line">        <span class="type">XCTAssertEqual</span>(<span class="number">50</span>, account.balance)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">func</span> <span class="title">testNegativeBalanceIsNotFine</span><span class="params">()</span></span> &#123;</span><br><span class="line">        account.deposit(<span class="number">50</span>)</span><br><span class="line">        account.withdraw(<span class="number">100</span>)</span><br><span class="line">        <span class="type">XCTAssertEqual</span>(<span class="number">0</span>, account.balance)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Don't forget to run all the tests, make sure it is succeeded.</p><h3 id="why-no-setup-and-teardown">Why no setup and tearDown</h3><p>People coming from <code>objc</code> may doubt that why the <code>account</code> instance is not put into <code>setUp</code> method, the way we use might cause different test cases sharing one instance variable, as we know, test cases should be independent with each other.</p><p>Yes, I had this doubt, too. So I did a test, by adding a "account balance should be 0" check before each test cases.</p><figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">testDepositAndWithdraw</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="type">XCTAssertEqual</span>(<span class="number">0</span>, account.balance)</span><br><span class="line">    account.deposit(<span class="number">100</span>)</span><br><span class="line">    account.withdraw(<span class="number">50</span>)</span><br><span class="line">    <span class="type">XCTAssertEqual</span>(<span class="number">50</span>, account.balance)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">testNegativeBalanceIsNotFine</span><span class="params">()</span></span> &#123;</span><br><span class="line">    <span class="type">XCTAssertEqual</span>(<span class="number">0</span>, account.balance)</span><br><span class="line">    account.deposit(<span class="number">50</span>)</span><br><span class="line">    account.withdraw(<span class="number">100</span>)</span><br><span class="line">    <span class="type">XCTAssertEqual</span>(<span class="number">0</span>, account.balance)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The result shows that the <code>XCTest</code> framework avoids instance variable sharing between test cases by instantiating a brand new <code>XCTestCase</code> object for each test case. That is, it instantiated two <code>SavingAccountTest</code> objects as our tests run.</p><h2 id="to-tdd-haters">To TDD Haters</h2><p>If you hate TDD, and may think this blog post is garbage.</p><p>Sorry for that, you can remove your browser history of this address, if it makes you feel better.</p><p>Also, I strongly recommend you to watch the "TDD dead" <a href="https://www.youtube.com/watch?v=z9quxZsLcfo">discussions</a> by <a href="http://martinfowler.com">Martin Fowler</a>, <a href="http://www.threeriversinstitute.org/Kent%20Beck.htm">Kent Beck</a> and <a href="http://david.heinemeierhansson.com">David Heinemeier Hansson</a>.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Long long ago, I wrote a post about &lt;a href=&quot;/2011/01/16/tdd-using-objective-c&quot;&gt;how to do TDD using Objective-C&lt;/a&gt;, since &lt;a href=&quot;http://www.apple.com/apple-events/june-2014/&quot;&gt;Apple WWDC 2014&lt;/a&gt;, &lt;a href=&quot;https://developer.apple.com/swift/&quot;&gt;Swift&lt;/a&gt; is really eye-catching, I think I should write a new one to follow the trend.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://developer.apple.com/library/ios/documentation/ToolsLanguages/Conceptual/Xcode_Overview/UnitTestYourApp/UnitTestYourApp.html&quot;&gt;XCTest&lt;/a&gt; is used as the unit test framework, and Xcode 6 is needed.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Niubei Mountain</title>
    <link href="http://blog.zhengdong.me/2014/04/24/niubei-mountain/"/>
    <id>http://blog.zhengdong.me/2014/04/24/niubei-mountain/</id>
    <published>2014-04-24T07:08:00.000Z</published>
    <updated>2020-12-02T14:59:19.914Z</updated>
    
    <content type="html"><![CDATA[<p><a><img src="/images/2014/niubeimountain1.jpg" width="560"></a> <a><img src="/images/2014/niubeimountain2.jpg" width="560"></a> <a><img src="/images/2014/niubeimountain3.jpg" width="560"></a> <a><img src="/images/2014/niubeimountain4.jpg" width="560"></a></p><p>-- Taken on April 12, 2014 (<a href="https://www.flickr.com/photos/don9z/albums/72157644027815816">flickr</a>)</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a&gt;&lt;img src=&quot;/images/2014/niubeimountain1.jpg&quot; width=&quot;560&quot;&gt;&lt;/a&gt; &lt;a&gt;&lt;img src=&quot;/images/2014/niubeimountain2.jpg&quot; width=&quot;560&quot;&gt;&lt;/a&gt; &lt;a&gt;&lt;img src=&quot;/images/2014/niubeimountain3.jpg&quot; width=&quot;560&quot;&gt;&lt;/a&gt; &lt;a&gt;&lt;img src=&quot;/images/2014/niubeimountain4.jpg&quot; width=&quot;560&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;-- Taken on April 12, 2014 (&lt;a href=&quot;https://www.flickr.com/photos/don9z/albums/72157644027815816&quot;&gt;flickr&lt;/a&gt;)&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>The Hadoop 2.x - Running a YARN Job (2)</title>
    <link href="http://blog.zhengdong.me/2014/02/26/the-hadoop-2-dot-x-running-a-yarn-job-2/"/>
    <id>http://blog.zhengdong.me/2014/02/26/the-hadoop-2-dot-x-running-a-yarn-job-2/</id>
    <published>2014-02-25T16:01:00.000Z</published>
    <updated>2020-11-28T10:52:32.827Z</updated>
    
    <content type="html"><![CDATA[<p>In <a href="/2014/02/25/the-hadoop-2-dot-x-running-a-yarn-job-1/">last blog post</a>, The job <code>Client</code> has been created and initialized.</p><p>This post will discuss on how does <code>Client</code> do to deploy the job to hadoop cluster.</p><p>Code snippets will be full of this post, to not confuse you, all comments added by me begin with <code>//**</code> instead of <code>//</code> or <code>/*</code> and the code can be cloned from <a href="http://git.apache.org/hadoop-common.git/">Apache Git Repository</a>, commit id is <code>2e01e27e5ba4ece19650484f646fac42596250ce</code>.</p><h2 id="the-resource-manager-proxy">The Resource Manager Proxy</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.applications.distributedshell.Client.java L330</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">run</span><span class="params">()</span> <span class="keyword">throws</span> IOException, YarnException </span>&#123;</span><br><span class="line"></span><br><span class="line">  LOG.info(<span class="string">&quot;Running Client&quot;</span>);</span><br><span class="line">  yarnClient.start();</span><br><span class="line"></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><p>At first, the <code>start</code> method of <code>yarnClient</code> is invoked. In <a href="/2014/02/25/the-hadoop-2-dot-x-running-a-yarn-job-1/">last blog post</a> we know, the <code>yarnClient</code> is actually a <code>YarnClientImpl</code> instance, but since it and its super <code>YarnClient</code> don't have the <code>start</code> interface implemented, the code path goes to <code>AbstractService</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.service.AbstractService.java L184</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">start</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (isInState(STATE.STARTED)) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//enter the started state</span></span><br><span class="line">  <span class="keyword">synchronized</span> (stateChangeLock) &#123;</span><br><span class="line">    <span class="keyword">if</span> (stateModel.enterState(STATE.STARTED) != STATE.STARTED) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        startTime = System.currentTimeMillis();</span><br><span class="line">        serviceStart();</span><br><span class="line">        <span class="keyword">if</span> (isInState(STATE.STARTED)) &#123;</span><br><span class="line">          <span class="comment">//if the service started (and isn&#x27;t now in a later state), notify</span></span><br><span class="line">          <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">            LOG.debug(<span class="string">&quot;Service &quot;</span> + getName() + <span class="string">&quot; is started&quot;</span>);</span><br><span class="line">          &#125;</span><br><span class="line">          notifyListeners();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        ...</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In above <code>start</code> method, the service state transfers from <code>STATE.INITED</code> to <code>STATE.STARTED</code>, and <code>serviceStart</code> method which implemented in <code>YarnClientImpl</code> is invoked.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.java L105</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceStart</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    rmClient = ClientRMProxy.createRMProxy(getConfig(),</span><br><span class="line">          ApplicationClientProtocol.class);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> YarnRuntimeException(e);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">super</span>.serviceStart();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The <code>ApplicationClientProtocol</code> instance <code>rmClient</code> is created in <code>serviceStart</code>.</p><p>When I step into the <code>createRMProxy</code> method, I was shocked, it is really a lot of code. For simplicity, this method creates a RPC proxy link to the Resource Manager, and communicates according to the <code>ApplicationClientProtocol</code>, the implementation is in <code>ApplicationClientProtocolPBClientImpl</code> class.</p><p>We'll discuss what's under the hood of Hadoop/YARN RPC framework in later posts, believe in me. :)</p><h2 id="the-protocol-buffer-protocols">The Protocol Buffer Protocols</h2><p>After the Resource Manager proxy is established, the <code>Client</code> begins to ask questions to Resource Manager.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.applications.distributedshell.Client.java L335</span></span><br><span class="line">YarnClusterMetrics clusterMetrics = yarnClient.getYarnClusterMetrics();</span><br><span class="line">LOG.info(<span class="string">&quot;Got Cluster metric info from ASM&quot;</span></span><br><span class="line">    + <span class="string">&quot;, numNodeManagers=&quot;</span> + clusterMetrics.getNumNodeManagers());</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.java L242</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> YarnClusterMetrics <span class="title">getYarnClusterMetrics</span><span class="params">()</span> <span class="keyword">throws</span> YarnException,</span></span><br><span class="line"><span class="function">  IOException </span>&#123;</span><br><span class="line">  GetClusterMetricsRequest request =</span><br><span class="line">    Records.newRecord(GetClusterMetricsRequest.class);</span><br><span class="line">  GetClusterMetricsResponse response = rmClient.getClusterMetrics(request);</span><br><span class="line">  <span class="keyword">return</span> response.getClusterMetrics();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>By the <code>rmClient</code> proxy, the <code>Client</code> queries cluster metrics, running nodes and queue information from Resource Manager. The code path is clear, creates the input parameter, passes to method call, and gets the result.</p><p>But you might want to ask, what does <code>Records.newRecord</code> do?</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.util.Records L30</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Records</span> </span>&#123;</span><br><span class="line">  <span class="comment">// The default record factory</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> RecordFactory factory =</span><br><span class="line">      RecordFactoryProvider.getRecordFactory(<span class="keyword">null</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function">T <span class="title">newRecord</span><span class="params">(Class&lt;T&gt; cls)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> factory.newRecordInstance(cls);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In <code>newRecord</code> method, by passing the <code>cls</code> parameter to <code>newRecordInstance</code> method call on <code>factory</code>, a new record is generated, with the <code>cls</code> type <code>GetClusterMetricsRequest</code>.</p><p>As we see in above code, the <code>factory</code> is created by calling <code>getRecordFactory</code> from <code>RecordFactoryProvider</code>.</p><p>Step into the <code>RecordFactoryProvider</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider.java L43</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> RecordFactory <span class="title">getRecordFactory</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line">  ...</span><br><span class="line">  String recordFactoryClassName = conf.get(</span><br><span class="line">      YarnConfiguration.IPC_RECORD_FACTORY_CLASS,</span><br><span class="line">      YarnConfiguration.DEFAULT_IPC_RECORD_FACTORY_CLASS);</span><br><span class="line">  <span class="keyword">return</span> (RecordFactory) getFactoryClassInstance(recordFactoryClassName);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Object <span class="title">getFactoryClassInstance</span><span class="params">(String factoryClassName)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    Class&lt;?&gt; clazz = Class.forName(factoryClassName);</span><br><span class="line">    Method method = clazz.getMethod(<span class="string">&quot;get&quot;</span>, <span class="keyword">null</span>);</span><br><span class="line">    method.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">return</span> method.invoke(<span class="keyword">null</span>, <span class="keyword">null</span>);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>If the <code>IPC_RECORD_FACTORY_CLASS</code> parameter is not set in configuration, the factory instance is created as <code>DEFAULT_IPC_RECORD_FACTORY_CLASS</code> which is "org.apache.hadoop.yarn.factories.impl.pb.RecordFactoryPBImpl" by calling its <code>get</code> method.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.factories.impl.pb.RecordFactoryPBImpl L44</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> RecordFactory <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> self;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The factory is got, then let's see how the <code>GetClusterMetricsRequest</code> instance is created by <code>newRecordInstance</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider.java L50</span></span><br><span class="line"><span class="keyword">public</span> &lt;T&gt; <span class="function">T <span class="title">newRecordInstance</span><span class="params">(Class&lt;T&gt; clazz)</span> </span>&#123;</span><br><span class="line">  Constructor&lt;?&gt; constructor = cache.get(clazz);</span><br><span class="line">  <span class="keyword">if</span> (constructor == <span class="keyword">null</span>) &#123;</span><br><span class="line">    Class&lt;?&gt; pbClazz = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      pbClazz = localConf.getClassByName(getPBImplClassName(clazz));</span><br><span class="line">    &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> YarnRuntimeException(<span class="string">&quot;Failed to load class: [&quot;</span></span><br><span class="line">          + getPBImplClassName(clazz) + <span class="string">&quot;]&quot;</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      constructor = pbClazz.getConstructor();</span><br><span class="line">      constructor.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">      cache.putIfAbsent(clazz, constructor);</span><br><span class="line">    &#125; <span class="keyword">catch</span> (NoSuchMethodException e) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> YarnRuntimeException(<span class="string">&quot;Could not find 0 argument constructor&quot;</span>, e);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    Object retObject = constructor.newInstance();</span><br><span class="line">    <span class="keyword">return</span> (T)retObject;</span><br><span class="line">  &#125; <span class="keyword">catch</span> (InvocationTargetException e) &#123;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> String <span class="title">getPBImplClassName</span><span class="params">(Class&lt;?&gt; clazz)</span> </span>&#123;</span><br><span class="line">  String srcPackagePart = getPackageName(clazz);</span><br><span class="line">  String srcClassName = getClassName(clazz);</span><br><span class="line">  String destPackagePart = srcPackagePart + <span class="string">&quot;.&quot;</span> + PB_IMPL_PACKAGE_SUFFIX;</span><br><span class="line">  String destClassPart = srcClassName + PB_IMPL_CLASS_SUFFIX;</span><br><span class="line">  <span class="keyword">return</span> destPackagePart + <span class="string">&quot;.&quot;</span> + destClassPart;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>At first, a class name is got from <code>getPBImplClassName</code>, since the parameter <code>clazz</code> is <code>GetClusterMetricsRequest</code>, the class name is <code>GetClusterMetricsRequestPBImpl</code>.</p><p>Then, gets the class constructor, and creates a new instance by calling below default constructor.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.api.protocolrecords.impl.pb.GetClusterMetricsRequestPBImpl.java L36</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">GetClusterMetricsRequestPBImpl</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  builder = GetClusterMetricsRequestProto.newBuilder();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>When you see the "Proto", "newBuilder", I think you might know now, it's <a href="https://developers.google.com/protocol-buffers/docs/javatutorial">Protocol Buffer</a>.</p><p>Now we get the <code>GetClusterMetricsRequest</code> instance as a parameter to the <code>rmClient</code> proxy.</p><p>By calling the <code>getClusterMetrics</code>, a <code>GetClusterMetricsResponse</code> instance is returned as a response.</p><p>Let's step into the real Resource Manager proxy implementation.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.java L146</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> GetClusterMetricsResponse <span class="title">getClusterMetrics</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    GetClusterMetricsRequest request)</span> <span class="keyword">throws</span> YarnException,</span></span><br><span class="line"><span class="function">    IOException </span>&#123;</span><br><span class="line">  GetClusterMetricsRequestProto requestProto =</span><br><span class="line">      ((GetClusterMetricsRequestPBImpl) request).getProto();</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> GetClusterMetricsResponsePBImpl(proxy.getClusterMetrics(<span class="keyword">null</span>,</span><br><span class="line">      requestProto));</span><br><span class="line">  &#125; <span class="keyword">catch</span> (ServiceException e) &#123;</span><br><span class="line">    RPCUtil.unwrapAndThrowException(e);</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The returned response is actually a <code>GetClusterMetricsResponsePBImpl</code> instance, in which has a <code>getClusterMetrics</code> interface implemented to translate the RPC result to native object.</p><p>Finally, the <code>getYarnClusterMetrics</code> method call on <code>yarnClient</code> is finished. Later method call such as <code>getNodeReports</code>, <code>getQueueInfo</code> is similar to <code>getYarnClusterMetrics</code>.</p><p>Let's continue the <code>run</code> method of <code>Client</code>.</p><h2 id="the-job-preparation">The Job Preparation</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.applications.distributedshell.Client.java L368</span></span><br><span class="line"><span class="comment">// Get a new application id</span></span><br><span class="line">YarnClientApplication app = yarnClient.createApplication();</span><br><span class="line">GetNewApplicationResponse appResponse = app.getNewApplicationResponse();</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>A <code>YarnClientApplication</code> instance is created through <code>yarnClient</code> proxy. If you really understand the above code flow for getting the cluster metrics, it is easy for you to look deep into the <code>createApplication</code> method.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.client.api.YarnClientApplication.java L38</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">YarnClientApplication</span><span class="params">(GetNewApplicationResponse newAppResponse,</span></span></span><br><span class="line"><span class="function"><span class="params">                             ApplicationSubmissionContext appContext)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.newAppResponse = newAppResponse;</span><br><span class="line">  <span class="keyword">this</span>.appSubmissionContext = appContext;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> GetNewApplicationResponse <span class="title">getNewApplicationResponse</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> newAppResponse;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> ApplicationSubmissionContext <span class="title">getApplicationSubmissionContext</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> appSubmissionContext;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>A <code>YarnClientApplication</code> has two instance variables, a <code>GetNewApplicationResponse</code> and a <code>ApplicationSubmissionContext</code> instance, the latter is used to store all needed information of a job, which will be submitted to the Resource Manager.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.applications.distributedshell.Client.java L369</span></span><br><span class="line"><span class="comment">//** Get appContext from YarnClientApplication instance</span></span><br><span class="line">ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();</span><br><span class="line"><span class="comment">//** Get appId from appContext</span></span><br><span class="line">ApplicationId appId = appContext.getApplicationId();</span><br><span class="line"><span class="comment">//** Set application name to appContext,</span></span><br><span class="line"><span class="comment">//** which will be used to submit the job later</span></span><br><span class="line">appContext.setApplicationName(appName);</span><br><span class="line"></span><br><span class="line"><span class="comment">//** Create a ContainerLaunchContextPBImpl instance,</span></span><br><span class="line"><span class="comment">//** which obeys the ContainerLaunchContextProto protocol</span></span><br><span class="line">ContainerLaunchContext amContainer = Records.newRecord(ContainerLaunchContext.class);</span><br><span class="line"></span><br><span class="line"><span class="comment">//** Create a local resouces map for the application master</span></span><br><span class="line">Map&lt;String, LocalResource&gt; localResources = <span class="keyword">new</span> HashMap&lt;String, LocalResource&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">//** Copy the application master jar to the hdfs</span></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line">Path src = <span class="keyword">new</span> Path(appMasterJar);</span><br><span class="line">String pathSuffix = appName + <span class="string">&quot;/&quot;</span> + appId.getId() + <span class="string">&quot;/AppMaster.jar&quot;</span>;</span><br><span class="line">...</span><br><span class="line"><span class="comment">//** Create a `LocalResourcePBImpl` instance, and add the AppMaster jar to it</span></span><br><span class="line">LocalResource amJarRsrc = Records.newRecord(LocalResource.class);</span><br><span class="line">...</span><br><span class="line">localResources.put(<span class="string">&quot;AppMaster.jar&quot;</span>,  amJarRsrc);</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">//** Copy the shell script to hdfs if exsits</span></span><br><span class="line">String shellPathSuffix = appName + <span class="string">&quot;/&quot;</span> + appId.getId() + <span class="string">&quot;/ExecShellScript.sh&quot;</span>;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">// Save local resource info into app master container launch context</span></span><br><span class="line">amContainer.setLocalResources(localResources);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Set the env variables to be setup in the env where the application master will be run</span></span><br><span class="line">Map&lt;String, String&gt; env = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span><br><span class="line">env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTLOCATION, hdfsShellScriptLocation);</span><br><span class="line">env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTTIMESTAMP, Long.toString(hdfsShellScriptTimestamp));</span><br><span class="line">env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTLEN, Long.toString(hdfsShellScriptLen));</span><br><span class="line"></span><br><span class="line"><span class="comment">//** Add AppMaster.jar location to classpath</span></span><br><span class="line">StringBuilder classPathEnv = <span class="keyword">new</span> StringBuilder(Environment.CLASSPATH.$())</span><br><span class="line">  .append(File.pathSeparatorChar).append(<span class="string">&quot;./*&quot;</span>);</span><br><span class="line">...</span><br><span class="line">env.put(<span class="string">&quot;CLASSPATH&quot;</span>, classPathEnv.toString());</span><br><span class="line"></span><br><span class="line"><span class="comment">//** Save env to amContainer</span></span><br><span class="line">amContainer.setEnvironment(env);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Set the necessary command to execute the application master</span></span><br><span class="line">Vector&lt;CharSequence&gt; vargs = <span class="keyword">new</span> Vector&lt;CharSequence&gt;(<span class="number">30</span>);</span><br><span class="line">vargs.add(Environment.JAVA_HOME.$() + <span class="string">&quot;/bin/java&quot;</span>);</span><br><span class="line">...</span><br><span class="line"><span class="comment">// Set params for Application Master</span></span><br><span class="line">vargs.add(<span class="string">&quot;--container_memory &quot;</span> + String.valueOf(containerMemory));</span><br><span class="line">vargs.add(<span class="string">&quot;--num_containers &quot;</span> + String.valueOf(numContainers));</span><br><span class="line">vargs.add(<span class="string">&quot;--priority &quot;</span> + String.valueOf(shellCmdPriority));</span><br><span class="line"><span class="keyword">if</span> (!shellCommand.isEmpty()) &#123;</span><br><span class="line">  vargs.add(<span class="string">&quot;--shell_command &quot;</span> + shellCommand + <span class="string">&quot;&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (!shellArgs.isEmpty()) &#123;</span><br><span class="line">  vargs.add(<span class="string">&quot;--shell_args &quot;</span> + shellArgs + <span class="string">&quot;&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">vargs.add(<span class="string">&quot;1&gt;&quot;</span> + ApplicationConstants.LOG_DIR_EXPANSION_VAR + <span class="string">&quot;/AppMaster.stdout&quot;</span>);</span><br><span class="line">vargs.add(<span class="string">&quot;2&gt;&quot;</span> + ApplicationConstants.LOG_DIR_EXPANSION_VAR + <span class="string">&quot;/AppMaster.stderr&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Get final commmand</span></span><br><span class="line">StringBuilder command = <span class="keyword">new</span> StringBuilder();</span><br><span class="line"><span class="keyword">for</span> (CharSequence str : vargs) &#123;</span><br><span class="line">  command.append(str).append(<span class="string">&quot; &quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">List&lt;String&gt; commands = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line">commands.add(command.toString());</span><br><span class="line"></span><br><span class="line"><span class="comment">//** Save command to amContainer</span></span><br><span class="line">amContainer.setCommands(commands);</span><br><span class="line">...</span><br><span class="line"><span class="comment">//** Save amContainer to appContext</span></span><br><span class="line">appContext.setAMContainerSpec(amContainer);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Set the priority for the application master</span></span><br><span class="line">Priority pri = Records.newRecord(Priority.class);</span><br><span class="line">pri.setPriority(amPriority);</span><br><span class="line">appContext.setPriority(pri);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Set the queue to which this application is to be submitted in the RM</span></span><br><span class="line">appContext.setQueue(amQueue);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Submit the application to the applications manager</span></span><br><span class="line"><span class="comment">//** Again, use the rmClient proxy</span></span><br><span class="line">yarnClient.submitApplication(appContext);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> monitorApplication(appId);</span><br></pre></td></tr></table></figure><p>Now the job is deployed to hadoop cluster, and the running status is fetched by <code>monitorApplication</code> through <code>rmClient</code> proxy at every 1 second.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;In &lt;a href=&quot;/2014/02/25/the-hadoop-2-dot-x-running-a-yarn-job-1/&quot;&gt;last blog post&lt;/a&gt;, The job &lt;code&gt;Client&lt;/code&gt; has been created and initialized.&lt;/p&gt;
&lt;p&gt;This post will discuss on how does &lt;code&gt;Client&lt;/code&gt; do to deploy the job to hadoop cluster.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>The Hadoop 2.x - Running a YARN Job (1)</title>
    <link href="http://blog.zhengdong.me/2014/02/25/the-hadoop-2-dot-x-running-a-yarn-job-1/"/>
    <id>http://blog.zhengdong.me/2014/02/25/the-hadoop-2-dot-x-running-a-yarn-job-1/</id>
    <published>2014-02-25T05:49:00.000Z</published>
    <updated>2020-11-28T10:52:32.826Z</updated>
    
    <content type="html"><![CDATA[<p>In last <a href="/2014/02/20/the-hadoop-2-dot-x-introduction-to-yarn/">blog post</a>, a hadoop distribution is built to run a YARN job.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop jar share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.2.0.jar \</span><br><span class="line">    org.apache.hadoop.yarn.applications.distributedshell.Client -jar \</span><br><span class="line">    share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.2.0.jar \</span><br><span class="line">    -shell_command <span class="string">&#x27;date&#x27;</span> -shell_args <span class="string">&quot;-u&quot;</span> -num_containers 2</span><br></pre></td></tr></table></figure><p>The <code>date -u</code> command is executed in Hadoop cluster by above script, we might conclude that there exists a dispatcher named <code>Client</code> in "hadoop-yarn-applications-distributedshell-2.2.0.jar", responsible for deploying a jar to cluster with parameters, such as shell command and args, and notify the cluster to execute the shell command.</p><p>To see what's in the rabbit hole, let's step into the <code>Client</code> source code.</p><p>Code snippets will be full of this post, to not confuse you, all comments added by me begin with <code>//**</code> instead of <code>//</code> or <code>/*</code> and the code can be cloned from <a href="http://git.apache.org/hadoop-common.git/">Apache Git Repository</a>, commit id is <code>2e01e27e5ba4ece19650484f646fac42596250ce</code>.</p><h2 id="the-process-logic">The Process Logic</h2><p>Since the <code>Client</code> is started as a process, we'd better to look into the <code>main</code> method first.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.applications.distributedshell.Client.java L164</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">boolean</span> result = <span class="keyword">false</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    Client client = <span class="keyword">new</span> Client();</span><br><span class="line">    LOG.info(<span class="string">&quot;Initializing Client&quot;</span>);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">boolean</span> doRun = client.init(args);</span><br><span class="line">      <span class="keyword">if</span> (!doRun) &#123;</span><br><span class="line">        System.exit(<span class="number">0</span>);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (IllegalArgumentException e) &#123;</span><br><span class="line">      ....</span><br><span class="line">    &#125;</span><br><span class="line">    result = client.run();</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>There are three procedures, first, constructs a <code>Client</code> instance, then initializes it, and invokes the <code>run</code> method of it.</p><h2 id="the-client-instance">The Client Instance</h2><p>There are three constructors in <code>Client</code>, the <code>main</code> method calls the default one with no parameters.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.applications.distributedshell.Client.java L227</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Client</span><span class="params">()</span> <span class="keyword">throws</span> Exception  </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>(<span class="keyword">new</span> YarnConfiguration());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The default constructor creates a <code>YarnConfiguration</code> instance and bypasses it to another constructor.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.applications.distributedshell.Client.java L194</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">Client</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> Exception  </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>(</span><br><span class="line">    <span class="string">&quot;org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster&quot;</span>,</span><br><span class="line">    conf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Then sets "org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster" as <code>appMasterClass</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.applications.distributedshell.Client.java L200</span></span><br><span class="line">Client(String appMasterMainClass, Configuration conf) &#123;</span><br><span class="line">  <span class="keyword">this</span>.conf = conf;</span><br><span class="line">  <span class="comment">//** Set appMasterMainClass</span></span><br><span class="line">  <span class="keyword">this</span>.appMasterMainClass = appMasterMainClass;</span><br><span class="line">  <span class="comment">//** This method call will create a YarnClientImpl instance</span></span><br><span class="line">  yarnClient = YarnClient.createYarnClient();</span><br><span class="line">  <span class="comment">//** Init the yarn client</span></span><br><span class="line">  yarnClient.init(conf);</span><br><span class="line">  <span class="comment">//** Create options for command parameters</span></span><br><span class="line">  <span class="comment">//** Will be parsed by GnuParser later</span></span><br><span class="line">  opts = <span class="keyword">new</span> Options();</span><br><span class="line">  opts.addOption(<span class="string">&quot;appname&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;Application Name. Default value - DistributedShell&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;priority&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;Application Priority. Default 0&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;queue&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;RM Queue in which this application is to be submitted&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;timeout&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;Application timeout in milliseconds&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;master_memory&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;Amount of memory in MB to be requested to run the application master&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;jar&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;Jar file containing the application master&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;shell_command&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;Shell command to be executed by the Application Master&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;shell_script&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;Location of the shell script to be executed&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;shell_args&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;Command line args for the shell script&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;shell_env&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;Environment for shell script. Specified as env_key=env_val pairs&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;shell_cmd_priority&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;Priority for the shell command containers&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;container_memory&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;Amount of memory in MB to be requested to run the shell command&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;num_containers&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;No. of containers on which the shell command needs to be executed&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;log_properties&quot;</span>, <span class="keyword">true</span>, <span class="string">&quot;log4j.properties file&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;debug&quot;</span>, <span class="keyword">false</span>, <span class="string">&quot;Dump out debug information&quot;</span>);</span><br><span class="line">  opts.addOption(<span class="string">&quot;help&quot;</span>, <span class="keyword">false</span>, <span class="string">&quot;Print usage&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The <code>YarnClientImpl</code> extends from <code>YarnClient</code> which extends from <code>AbstractService</code> which implements from <code>Service</code>, the main job of it is to control the service life-cycle,</p><p>The <code>YarnClientImpl</code> is created and initialized.</p><p>Since the <code>init</code> method can't be found in <code>YarnClientImpl</code> as well as <code>YarnClient</code>, the actual <code>init</code> happens in <code>AbstractService</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.service.AbstractService.java L151</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(Configuration conf)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (conf == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> ServiceStateException(<span class="string">&quot;Cannot initialize service &quot;</span></span><br><span class="line">                                    + getName() + <span class="string">&quot;: null configuration&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (isInState(STATE.INITED)) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">synchronized</span> (stateChangeLock) &#123;</span><br><span class="line">    <span class="keyword">if</span> (enterState(STATE.INITED) != STATE.INITED) &#123;</span><br><span class="line">      setConfig(conf);</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        serviceInit(config);</span><br><span class="line">        <span class="keyword">if</span> (isInState(STATE.INITED)) &#123;</span><br><span class="line">          <span class="comment">//if the service ended up here during init,</span></span><br><span class="line">          <span class="comment">//notify the listeners</span></span><br><span class="line">          notifyListeners();</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        noteFailure(e);</span><br><span class="line">        ServiceOperations.stopQuietly(LOG, <span class="keyword">this</span>);</span><br><span class="line">        <span class="keyword">throw</span> ServiceStateException.convert(e);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>It first checks whether the state <code>isInState</code> <code>STATE.INITED</code>, if not, <code>enterState(STATE.INITED)</code>, and calls the <code>serviceInit</code> method, when the state is successfully transferred to <code>STATE.INITED</code>, <code>notifyListeners()</code> is called.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.service.AbstractService.java L415</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">notifyListeners</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    listeners.notifyListeners(<span class="keyword">this</span>);</span><br><span class="line">    globalListeners.notifyListeners(<span class="keyword">this</span>);</span><br><span class="line">  &#125; <span class="keyword">catch</span> (Throwable e) &#123;</span><br><span class="line">    LOG.warn(<span class="string">&quot;Exception while notifying listeners of &quot;</span> + <span class="keyword">this</span> + <span class="string">&quot;: &quot;</span> + e,</span><br><span class="line">             e);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//** org.apache.hadoop.service.ServiceOperations.java L139</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">notifyListeners</span><span class="params">(Service service)</span> </span>&#123;</span><br><span class="line">  <span class="comment">//take a very fast snapshot of the callback list</span></span><br><span class="line">  <span class="comment">//very much like CopyOnWriteArrayList, only more minimal</span></span><br><span class="line">  ServiceStateChangeListener[] callbacks;</span><br><span class="line">  <span class="keyword">synchronized</span> (<span class="keyword">this</span>) &#123;</span><br><span class="line">    callbacks = listeners.toArray(<span class="keyword">new</span> ServiceStateChangeListener[listeners.size()]);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">//iterate through the listeners outside the synchronized method,</span></span><br><span class="line">  <span class="comment">//ensuring that listener registration/unregistration doesn&#x27;t break anything</span></span><br><span class="line">  <span class="keyword">for</span> (ServiceStateChangeListener l : callbacks) &#123;</span><br><span class="line">    l.stateChanged(service);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>notifyListeners</code> notifies all its listeners and global listeners to change their states correspondingly.</p><p>But, what is the STATE?</p><h2 id="the-state-model">The State Model</h2><p>Let's go back to the <code>YarnClient.createYarnClient()</code> method.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.client.api.YarnClient.java L55</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> YarnClient <span class="title">createYarnClient</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  YarnClient client = <span class="keyword">new</span> YarnClientImpl();</span><br><span class="line">  <span class="keyword">return</span> client;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The <code>YarnClientImpl</code> instance is created.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.java L86</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">YarnClientImpl</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">super</span>(YarnClientImpl.class.getName());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.client.api.YarnClient.java L60</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="title">YarnClient</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">super</span>(name);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.service.AbstractService.java L111</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">AbstractService</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.name = name;</span><br><span class="line">  stateModel = <span class="keyword">new</span> ServiceStateModel(name);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Bingo, that's the state model: <code>ServiceStateModel</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.service.ServiceStateModel.java L66</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ServiceStateModel</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>(name, Service.STATE.NOTINITED);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">ServiceStateModel</span><span class="params">(String name, Service.STATE state)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.state = state;</span><br><span class="line">  <span class="keyword">this</span>.name = name;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The state model is simply a name state pair, the name is the service implementation class name.</p><p>The <code>isInState</code> checks the state value.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.service.ServiceStateModel.java L84</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isInState</span><span class="params">(Service.STATE proposed)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> state.equals(proposed);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The <code>enterState</code> changes the state value after <code>checkStateTransition</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.service.ServiceStateModel.java L110</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">synchronized</span> Service.<span class="function">STATE <span class="title">enterState</span><span class="params">(Service.STATE proposed)</span> </span>&#123;</span><br><span class="line">  checkStateTransition(name, state, proposed);</span><br><span class="line">  Service.STATE oldState = state;</span><br><span class="line">  <span class="comment">//atomic write of the new state</span></span><br><span class="line">  state = proposed;</span><br><span class="line">  <span class="keyword">return</span> oldState;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The state transferring is checked by looking up the statemap with current state, then return a boolean to indicate whether the state transition is valid, if it's invalid, the <code>checkStateTransition</code> throws <code>ServiceStateException</code>.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.service.ServiceStateModel.java L125</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">checkStateTransition</span><span class="params">(String name,</span></span></span><br><span class="line"><span class="function"><span class="params">                                        Service.STATE state,</span></span></span><br><span class="line"><span class="function"><span class="params">                                        Service.STATE proposed)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (!isValidStateTransition(state, proposed)) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> ServiceStateException(name + <span class="string">&quot; cannot enter state &quot;</span></span><br><span class="line">                                    + proposed + <span class="string">&quot; from state &quot;</span> + state);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isValidStateTransition</span><span class="params">(Service.STATE current,</span></span></span><br><span class="line"><span class="function"><span class="params">                                             Service.STATE proposed)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">boolean</span>[] row = statemap[current.getValue()];</span><br><span class="line">  <span class="keyword">return</span> row[proposed.getValue()];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Then what's in the statemap?</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.service.ServiceStateModel.java L35</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span>[][] statemap =</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//                uninited inited started stopped</span></span><br><span class="line">    <span class="comment">/* uninited  */</span>    &#123;<span class="keyword">false</span>, <span class="keyword">true</span>,  <span class="keyword">false</span>,  <span class="keyword">true</span>&#125;,</span><br><span class="line">    <span class="comment">/* inited    */</span>    &#123;<span class="keyword">false</span>, <span class="keyword">true</span>,  <span class="keyword">true</span>,   <span class="keyword">true</span>&#125;,</span><br><span class="line">    <span class="comment">/* started   */</span>    &#123;<span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">true</span>,   <span class="keyword">true</span>&#125;,</span><br><span class="line">    <span class="comment">/* stopped   */</span>    &#123;<span class="keyword">false</span>, <span class="keyword">false</span>, <span class="keyword">false</span>,  <span class="keyword">true</span>&#125;,</span><br><span class="line">  &#125;;</span><br></pre></td></tr></table></figure><p>That's the state model we are looking for. The current state is the row index, the proposed state is the column index, the value is whether the current state can be transfered to proposed state.</p><h2 id="the-command-initialization">The Command Initialization</h2><p>Go back again, to when the <code>YarnClientImpl</code> is about to be initialized, the <code>init</code> method of its super <code>AbstractService</code> calls the <code>serviceInit</code> method and the state is transfered to <code>STATE.INITED</code>.</p><p><code>YarnClientImpl</code> has implemented the <code>serviceInit</code> interface.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.java L96</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">serviceInit</span><span class="params">(Configuration conf)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>.rmAddress = getRmAddress(conf);</span><br><span class="line">  statePollIntervalMillis = conf.getLong(</span><br><span class="line">      YarnConfiguration.YARN_CLIENT_APP_SUBMISSION_POLL_INTERVAL_MS,</span><br><span class="line">      YarnConfiguration.DEFAULT_YARN_CLIENT_APP_SUBMISSION_POLL_INTERVAL_MS);</span><br><span class="line">  <span class="keyword">super</span>.serviceInit(conf);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The address of Resource Manager, <code>rmAddress</code> is assigned from the configuration instance.</p><p>Now the <code>Client</code> instance is created successfully, <code>init</code> method is invoked by <code>main</code> to initialize the instance.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//** org.apache.hadoop.yarn.applications.distributedshell.java L244</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">init</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ParseException </span>&#123;</span><br><span class="line"></span><br><span class="line">  CommandLine cliParser = <span class="keyword">new</span> GnuParser().parse(opts, args);</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  appName = cliParser.getOptionValue(<span class="string">&quot;appname&quot;</span>, <span class="string">&quot;DistributedShell&quot;</span>);</span><br><span class="line">  amPriority = Integer.parseInt(cliParser.getOptionValue(<span class="string">&quot;priority&quot;</span>, <span class="string">&quot;0&quot;</span>));</span><br><span class="line">  amQueue = cliParser.getOptionValue(<span class="string">&quot;queue&quot;</span>, <span class="string">&quot;default&quot;</span>);</span><br><span class="line">  amMemory = Integer.parseInt(cliParser.getOptionValue(<span class="string">&quot;master_memory&quot;</span>, <span class="string">&quot;10&quot;</span>));</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  appMasterJar = cliParser.getOptionValue(<span class="string">&quot;jar&quot;</span>);</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  shellCommand = cliParser.getOptionValue(<span class="string">&quot;shell_command&quot;</span>);</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The command line options is parsed, and assigned to instance variables for later usage.</p><p>The <code>Client</code> is ready to <code>run</code>.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;In last &lt;a href=&quot;/2014/02/20/the-hadoop-2-dot-x-introduction-to-yarn/&quot;&gt;blog post&lt;/a&gt;, a hadoop distribution is built to run a YARN job.&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ bin/hadoop jar share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.2.0.jar \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    org.apache.hadoop.yarn.applications.distributedshell.Client -jar \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.2.0.jar \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    -shell_command &lt;span class=&quot;string&quot;&gt;&amp;#x27;date&amp;#x27;&lt;/span&gt; -shell_args &lt;span class=&quot;string&quot;&gt;&amp;quot;-u&amp;quot;&lt;/span&gt; -num_containers 2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>The Hadoop 2.x - Introduction to YARN</title>
    <link href="http://blog.zhengdong.me/2014/02/20/the-hadoop-2-dot-x-introduction-to-yarn/"/>
    <id>http://blog.zhengdong.me/2014/02/20/the-hadoop-2-dot-x-introduction-to-yarn/</id>
    <published>2014-02-20T15:00:00.000Z</published>
    <updated>2020-11-28T10:52:32.825Z</updated>
    
    <content type="html"><![CDATA[<h2 id="the-old-mapreduce">The Old MapReduce</h2><p>The Hadoop 0.x MapReduce system composed of JobTracker and TaskTrackers.</p><p><img src="/images/2014/introtoyarn-jt.png" width="560"></p><p>The JobTracker is responsible for resource management, tracking resource usage and job life-cycle management, e.g. scheduling job tasks, tracking progress, providing fault-tolerance for tasks.</p><p>The TaskTracker is the per-node slave for JobTracker, takes orders from the JobTracker to launch or tear-down tasks, and provides task status information to the JobTracker periodically.</p><p>For those years, we are benefited from the MapReduce framework, it's the most successful programming model in the big data world.</p><p>But MapReduce is not everything, we need to do graph processing, or real-time stream processing, since Hadoop is essentially batch oriented, we have to look for other systems to do those work.</p><p>And the hadoop community made a huge change.</p><h2 id="the-hadoop-yarn">The Hadoop YARN</h2><p>The fundamental idea of YARN is to split up the two major responsibilities of the JobTracker i.e. resource management and job scheduling/monitoring, into separate daemons: a global ResourceManager (RM) and per-application ApplicationMaster (AM).</p><p><img src="/images/2014/introtoyarn-rm.png" width="560"></p><p>The ResourceManager is responsible for allocating resources to the running applications.</p><p>The NodeManager is a per-machine slave, works on launching the application's containers, monitoring the resource usage, and reporting them to the ResourceManager.</p><p>The ApplicationMaster is a per-application framework, which runs as a normal container, responsible for negotiating appropriate resource containers from ResourceManager, tracking their status and monitoring for progress.</p><p>Click <a href="http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/">here</a> to read the details about Hadoop YARN.</p><h2 id="the-hadoop-distribution">The Hadoop Distribution</h2><p>My intention is to read the source code of hadoop, so I prefer to build a hadoop distribution from the source code.</p><p>First, git clone from <a href="https://github.com/apache/hadoop-common">github</a> or <a href="http://git.apache.org/hadoop-common.git/">apache</a>.</p><p>Second, checkout the branch-2.2.0 branch, which is quite stable.</p><p>Third, apply below patch,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">diff --git a&#x2F;hadoop-common-project&#x2F;hadoop-auth&#x2F;pom.xml b&#x2F;hadoop-common-project&#x2F;hadoop-auth&#x2F;pom.xml</span><br><span class="line">index 8819941..70ff207 100644</span><br><span class="line">--- a&#x2F;hadoop-common-project&#x2F;hadoop-auth&#x2F;pom.xml</span><br><span class="line">+++ b&#x2F;hadoop-common-project&#x2F;hadoop-auth&#x2F;pom.xml</span><br><span class="line">@@ -55,6 +55,11 @@</span><br><span class="line">     &lt;&#x2F;dependency&gt;</span><br><span class="line">     &lt;dependency&gt;</span><br><span class="line">       &lt;groupId&gt;org.mortbay.jetty&lt;&#x2F;groupId&gt;</span><br><span class="line">+      &lt;artifactId&gt;jetty-util&lt;&#x2F;artifactId&gt;</span><br><span class="line">+      &lt;scope&gt;test&lt;&#x2F;scope&gt;</span><br><span class="line">+    &lt;&#x2F;dependency&gt;</span><br><span class="line">+    &lt;dependency&gt;</span><br><span class="line">+      &lt;groupId&gt;org.mortbay.jetty&lt;&#x2F;groupId&gt;</span><br><span class="line">       &lt;artifactId&gt;jetty&lt;&#x2F;artifactId&gt;</span><br><span class="line">       &lt;scope&gt;test&lt;&#x2F;scope&gt;</span><br><span class="line">     &lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure><p>Fourth, type and run.</p><p><code>mvn package -Pdist -DskipTests -Dtar</code></p><h2 id="the-installation-guide">The Installation Guide</h2><p><a href="http://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide">This</a> is a great guide to install Hadoop 2.2.0.</p><p>I did a single installation, after configuring everything, hdfs can be setup and daemons are started by below scripts.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ /bin/hdfs namenode -format</span><br><span class="line">$ sbin/hadoop-daemon.sh start namenode</span><br><span class="line">$ sbin/hadoop-daemon.sh start datanode</span><br><span class="line">$ sbin/yarn-daemon.sh start resourcemanager</span><br><span class="line">$ sbin/yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure><p>Then, run <code>date -u</code> commands on two containers.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hadoop jar share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.2.0.jar org.apache.hadoop.yarn.applications.distributedshell.Client -jar share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.2.0.jar -shell_command <span class="string">&#x27;date&#x27;</span> -shell_args <span class="string">&quot;-u&quot;</span> -num_containers 2</span><br></pre></td></tr></table></figure><p>The logs are printed:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">14/02/20 22:50:59 INFO distributedshell.Client: Initializing Client</span><br><span class="line">14/02/20 22:50:59 INFO distributedshell.Client: Running Client</span><br><span class="line">14/02/20 22:50:59 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032</span><br><span class="line">14/02/20 22:50:59 INFO distributedshell.Client: Got Cluster metric info from ASM, numNodeManagers=1</span><br><span class="line">14/02/20 22:50:59 INFO distributedshell.Client: Got Cluster node info from ASM</span><br><span class="line">14/02/20 22:50:59 INFO distributedshell.Client: Got node report from ASM <span class="keyword">for</span>, nodeId=192.168.0.102:52786, nodeAddress192.168.0.102:8042, nodeRackName/default-rack, nodeNumContainers0</span><br><span class="line">14/02/20 22:50:59 INFO distributedshell.Client: Queue info, queueName=default, queueCurrentCapacity=0.0, queueMaxCapacity=1.0, queueApplicationCount=0, queueChildQueueCount=0</span><br><span class="line">14/02/20 22:50:59 INFO distributedshell.Client: User ACL Info <span class="keyword">for</span> Queue, queueName=root, userAcl=SUBMIT_APPLICATIONS</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">14/02/20 22:50:59 INFO distributedshell.Client: Max mem capabililty of resources <span class="keyword">in</span> this cluster 2048</span><br><span class="line">14/02/20 22:50:59 INFO distributedshell.Client: Copy App Master jar from <span class="built_in">local</span> filesystem and add to <span class="built_in">local</span> environment</span><br><span class="line">14/02/20 22:51:00 INFO distributedshell.Client: Set the environment <span class="keyword">for</span> the application master</span><br><span class="line">14/02/20 22:51:00 INFO distributedshell.Client: Setting up app master <span class="built_in">command</span></span><br><span class="line">14/02/20 22:51:00 INFO distributedshell.Client: Completed setting up app master <span class="built_in">command</span> <span class="variable">$JAVA_HOME</span>/bin/java -Xmx10m org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster --container_memory 10 --num_containers 2 --priority 0 --shell_command date --shell_args -u 1&gt;&lt;LOG_DIR&gt;/AppMaster.stdout 2&gt;&lt;LOG_DIR&gt;/AppMaster.stderr</span><br><span class="line">14/02/20 22:51:00 INFO distributedshell.Client: Submitting application to ASM</span><br><span class="line">14/02/20 22:51:00 INFO impl.YarnClientImpl: Submitted application application_1392907840296_0001 to ResourceManager at /0.0.0.0:8032</span><br><span class="line">14/02/20 22:51:01 INFO distributedshell.Client: Got application report from ASM <span class="keyword">for</span>, appId=1, clientToAMToken=null, appDiagnostics=, appMasterHost=N/A, appQueue=default, appMasterRpcPort=0, appStartTime=1392907860335, yarnAppState=ACCEPTED, distributedFinalState=UNDEFINED, appTrackingUrl=192.168.0.102:8088/proxy/application_1392907840296_0001/, appUser=chris</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">14/02/20 22:51:08 INFO distributedshell.Client: Application has completed successfully. Breaking monitoring loop</span><br><span class="line">14/02/20 22:51:08 INFO distributedshell.Client: Application completed successfully</span><br></pre></td></tr></table></figure><p>And the results are:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cat logs/userlogs/application_1392907840296_0001/*/stdout</span><br><span class="line">Thu Feb 20 14:51:05 UTC 2014</span><br><span class="line">Thu Feb 20 14:51:06 UTC 2014</span><br></pre></td></tr></table></figure><p>That's my first YARN job running!</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;the-old-mapreduce&quot;&gt;The Old MapReduce&lt;/h2&gt;
&lt;p&gt;The Hadoop 0.x MapReduce system composed of JobTracker and TaskTrackers.&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Colorful Season</title>
    <link href="http://blog.zhengdong.me/2013/11/30/colorful-season/"/>
    <id>http://blog.zhengdong.me/2013/11/30/colorful-season/</id>
    <published>2013-11-30T13:11:00.000Z</published>
    <updated>2020-12-02T15:33:08.382Z</updated>
    
    <content type="html"><![CDATA[<a><img src="/images/2013/colorfulseason1.jpg" width="560"></a> <a><img src="/images/2013/colorfulseason2.jpg" width="560"></a> <a><img src="/images/2013/colorfulseason3.jpg" width="560"></a> <a><img src="/images/2013/colorfulseason4.jpg" width="560"></a><center><img src="/images/2013/colorfulseason5.jpg" width="280" style="display: inline;"> <img src="/images/2013/colorfulseason6.jpg" width="280" style="display: inline;"></center><p>-- Taken on November 30, 2013 (<a href="https://www.flickr.com/photos/don9z/albums/72157638207745344">flickr</a>)</p>]]></content>
    
    
    <summary type="html">&lt;a&gt;&lt;img src=&quot;/images/2013/colorfulseason1.jpg&quot; width=&quot;560&quot;&gt;&lt;/a&gt; &lt;a&gt;&lt;img src=&quot;/images/2013/colorfulseason2.jpg&quot; width=&quot;560&quot;&gt;&lt;/a&gt;</summary>
    
    
    
    
  </entry>
  
</feed>
